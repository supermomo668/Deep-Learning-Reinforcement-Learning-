{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odNaDE1zyrL2"
   },
   "source": [
    "# install dependancies, takes around 45 seconds\n",
    "\n",
    "Rendering Dependancies\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "8-AxnvAVyzQQ"
   },
   "source": [
    "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
    "#!pip install gym pyvirtualdisplay 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A-1LTSH88EE"
   },
   "source": [
    "Pacman Dependancies"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "TCelFzWY9MBI",
    "outputId": "6f32734e-4791-49d0-c7c9-1ac7853ee167"
   },
   "source": [
    "# !apt-get update > /dev/null 2>&1\n",
    "# !apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1\n",
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APXSx7hg19TH"
   },
   "source": [
    "# Imports and Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OtnCDULP74i1"
   },
   "outputs": [],
   "source": [
    "import sys, os, copy\n",
    "from pathlib import Path\n",
    "proj_folder = Path('.').absolute()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQEtc28G4niA",
    "outputId": "eb9d76c5-7e1f-441f-b2ec-70fa69fcfb98"
   },
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "G9UWeToN4r7D"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env, save_path=proj_folder/'video'):\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    env = record_video.RecordVideo(env, proj_folder/'video', name_prefix='train')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3BGbWOu179M"
   },
   "source": [
    "# Run Simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Ys9oSdMk7e64"
   },
   "outputs": [],
   "source": [
    "import numpy as np, torch, gym\n",
    "import torch.nn.functional as F\n",
    "from gym.wrappers import record_video\n",
    "#\n",
    "from gym import logger as gymlogger\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "global DEBUG\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cdEOSetG7h0L"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation, layers=[32,32,16]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_size, layers[0])\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(layers[0], layers[1])\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(layers[1], layers[2])\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(layers[2], output_size)\n",
    "        self.output_activation = activation\n",
    "\n",
    "        #initialize weights, following 'fan_avg' approach\n",
    "        torch.nn.init.xavier_normal_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear3.weight)\n",
    "        torch.nn.init.xavier_normal_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.linear1(inputs))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        x = self.output_activation(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "    def fit(self, loss, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "class A2C(object):\n",
    "    # Implementation of N-step Advantage Actor Critic.\n",
    "\n",
    "    def __init__(self, actor, actor_lr, N, nA, critic, critic_lr, baseline=False, a2c=True):\n",
    "        # Note: baseline is true if we use reinforce with baseline\n",
    "        #       a2c is true if we use a2c else reinforce\n",
    "        \n",
    "        # TODO: Initializes A2C.\n",
    "        self.type = \"A2C\" if a2c else (\"Baseline\" if baseline else \"Reinforce\")  # Pick one of: \"A2C\", \"Baseline\", \"Reinforce\"\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        actor.to(device)\n",
    "        # define meta variabless\n",
    "        self.save_dir = Path('./').absolute()/'Output'\n",
    "        if self.type == \"A2C\":\n",
    "            critic.to(device)\n",
    "            self.critic_optimizer = torch.optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "        elif self.type == \"Baseline\":\n",
    "            pass\n",
    "        else:\n",
    "            # Reinforce\n",
    "            pass\n",
    "        self.actor = actor\n",
    "        self.actor_optimizer = torch.optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "        self.N = N\n",
    "        assert self.type is not None, \"Type must be provided\"\n",
    "\n",
    "    def reinforce_criterion(self, y_pred, y_true):\n",
    "        log_y_pred = torch.log(y_pred)   # log probability of ation\n",
    "        l = torch.multiply(log_y_pred, y_true)   # (n_eps, nA)\n",
    "        l_action = torch.sum(l, axis=1)  # (n_eps,)\n",
    "        return torch.mean(l_action)\n",
    "    \n",
    "    def fit_actor(self, states, G_total, epochs=1, batch_size=1):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.actor.train()\n",
    "        n_example = len(states)\n",
    "        history = dict.fromkeys(['loss'],[])\n",
    "        for e in range(epochs):\n",
    "            for i in range(int(np.ceil(len(states)/batch_size))):\n",
    "                start_idx, end_idx = batch_size*i, min(batch_size*(i+1), n_example)\n",
    "                states_, target_ = states[start_idx: end_idx], G_total[start_idx: end_idx]\n",
    "                pred_G = self.actor(states_.cuda())\n",
    "                loss = self.reinforce_criterion(pred_G, target_.cuda())\n",
    "                # # measure metrics and record loss\n",
    "                # m1 = metric1(cls_out.cpu(), target)\n",
    "                # m2 = metric2(cls_out.cpu(), target)\n",
    "                history['loss'].append(loss)\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                if DEBUG: print(f\"Latest Loss:{history['loss'][-1]}\")\n",
    "        return history\n",
    "\n",
    "    def evaluate_policy(self, env):\n",
    "        # TODO: Compute Accumulative trajectory reward(set a trajectory length threshold if you want)\n",
    "        \"\"\" compute rewards\n",
    "        \"\"\"\n",
    "        self.actor.eval()\n",
    "        _, _, r, _ = self.generate_episode(env, render=False)\n",
    "        rtot = np.sum(np.array(r))\n",
    "        return rtot\n",
    "\n",
    "    def generate_episode(self, env, render=False):\n",
    "        \"\"\"\n",
    "\t\t# Generates an episode by executing the current policy in the given env.\n",
    "\t\t# Returns:\n",
    "\t\t# - a list of states, indexed by time step\n",
    "            shape: (t)\n",
    "\t\t# - a list of actions, indexed by time step\n",
    "            shape: (t, nA)\n",
    "\t\t# - a list of rewards, indexed by time step\n",
    "            shape: (t)\n",
    "        \"\"\"\n",
    "        states, actions, rewards, actions_probs=[], [] ,[], []\n",
    "        # \n",
    "        nS = env.observation_space.shape[0]\n",
    "        nA = env.action_space.n\n",
    "\t\t# Start episode\n",
    "        states.append(np.expand_dims(env.reset(), axis=0))\n",
    "        terminal = False\n",
    "        if render: \n",
    "            env.render(mode='rgb_array')\n",
    "        cts = 0\n",
    "        while not terminal:\n",
    "            cts+=1\n",
    "            ac = self.actor(torch.Tensor(states[-1]).to(device)).squeeze(0)   # ensure [nA] vector\n",
    "            if DEBUG: print(f\"Input state:{states[-1]}\\nOutput action:{ac}\")\n",
    "            ac_prob = ac.detach().cpu().numpy().flatten()\n",
    "            ac_prob = np.nan_to_num(ac_prob,0)\n",
    "            #\n",
    "            a_ = np.random.choice(ac_prob, p=ac_prob)   # stochastic choice\n",
    "            curr_ac = np.where(ac_prob==a_)[0][0]  # Current Action\n",
    "            action_OH = np.eye(nA)[curr_ac]  # one-hot action\n",
    "            \n",
    "            # move in direction and get environment output\n",
    "            s, r, terminal, _ = env.step(curr_ac)\n",
    "            # add to history\n",
    "            states.append(np.expand_dims(s, axis=0))\n",
    "            actions.append(action_OH)\n",
    "            actions_probs.append(ac_prob)\n",
    "            rewards.append(r)\n",
    "            curr_state = copy.deepcopy(s)\n",
    "        if DEBUG: print(f\"action probs:{ac_prob}\\nfinal action:{action_OH}\")\n",
    "        print(\"Finished after {} timesteps\".format(cts+1))\n",
    "\t\t# flatten \n",
    "        states=np.reshape(np.array(states), (-1, nS))\n",
    "        actions=np.reshape(np.array(actions), (-1, nA))\n",
    "        return np.stack(states), np.stack(actions), np.stack(rewards), np.stack(actions_probs)\n",
    "\n",
    "    def train(self, env, gamma=0.99, n=10):\n",
    "        \"\"\"\n",
    "        # Trains the model on a single episode using REINFORCE or A2C/A3C.\n",
    "        params:\n",
    "            n: number of n-steps look ahead\n",
    "        \"\"\"\n",
    "        # TODO: Implement this method. It may be helpful to call the class\n",
    "        #       method generate_episode() to generate training data.\n",
    "        eps_lim = 500    # length of episode 'limit'\n",
    "        batch_size_ratio = 1    # portion of length of states to feed in NN\n",
    "        alpha = 1e-2\n",
    "        assert 0<batch_size_ratio<=1, \"Batch size between 0 and 1\"\n",
    "        #\n",
    "        env = wrap_env(env)\n",
    "        mean_total, std_total = [], []\n",
    "        # generate episode\n",
    "        states, actions, rewards, actions_prob = self.generate_episode(env, render=True)\n",
    "        states = states[:-1]   # remove the last state\n",
    "        # get discounted reward vector\n",
    "            # [sum : r*gamma*0...r*gamma*n]\n",
    "        G_tot=[0]\n",
    "        for n_gamma, r in enumerate(reversed(rewards)):\n",
    "            # insert from last reward to beginning \n",
    "            G_tot.insert(0, r+gamma**n_gamma*G_tot[0])\n",
    "        if DEBUG: print(f\"Sum of Expected Rewards G:{G_tot}\")\n",
    "        G_tot=np.array(G_tot[:-1])\n",
    "        # weight actions by rewards and fit model\n",
    "        if DEBUG: print(f\"G_tot:[{len(G_tot)}], actions:[{len(actions)}], states:[{len(states)}]\")\n",
    "        G_total_actions = np.multiply(G_tot, actions.T).T   # (t, nA)\n",
    "        train_history = self.fit_actor(torch.Tensor(states), torch.Tensor(G_total_actions*alpha), epochs=1, batch_size=int(len(states)*batch_size_ratio))\n",
    "        return train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqrh5P-cCbe3"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xKD2PlR3WJii"
   },
   "outputs": [],
   "source": [
    "import argparse, matplotlib.pyplot as plt, tqdm\n",
    "def parse_a2c_arguments():\n",
    "    # Command-line flags are defined here.\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env-name', dest='env_name', type=str,\n",
    "                        default='CartPole-v0', help=\"Name of the environment to be run.\")   # 'LunarLander-v2'\n",
    "    parser.add_argument('--num-episodes', dest='num_episodes', type=int,\n",
    "                        default=100, help=\"Number of episodes to train on.\")\n",
    "    parser.add_argument('--lr', dest='lr', type=float,\n",
    "                        default=5e-4, help=\"The actor's learning rate.\")\n",
    "    parser.add_argument('--baseline-lr', dest='baseline_lr', type=float,\n",
    "                        default=5e-4, help=\"The actor's learning rate.\")\n",
    "    parser.add_argument('--critic-lr', dest='critic_lr', type=float,\n",
    "                        default=1e-4, help=\"The critic's learning rate.\")\n",
    "    parser.add_argument('--n', dest='n', type=int,\n",
    "                        default=100, help=\"The value of N in N-step A2C.\")\n",
    "\n",
    "    parser_group = parser.add_mutually_exclusive_group(required=False)\n",
    "    parser_group.add_argument('--render', dest='render',\n",
    "                              action='store_true',\n",
    "                              help=\"Whether to render the environment.\")\n",
    "    parser_group.add_argument('--no-render', dest='render',\n",
    "                              action='store_false',\n",
    "                              help=\"Whether to render the environment.\")\n",
    "    parser.set_defaults(render=False)\n",
    "\n",
    "    return parser.parse_known_args()[0]    #.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Zj2YirE8nIB",
    "outputId": "e4a6e959-5eab-4606-9037-887c8f73b36f",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda\\Anaconda3\\envs\\ptml\\lib\\site-packages\\gym\\wrappers\\record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at C:\\Users\\Mo\\OneDrive\\Notes\\CMU\\10703 - Deep RL\\F22_10703_Homework_2\\hw2_code\\pytorch\\video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations:Namespace(baseline_lr=0.0005, critic_lr=0.0001, env_name='CartPole-v0', lr=0.0005, n=100, num_episodes=100, render=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda\\Anaconda3\\envs\\ptml\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 15 timesteps\n",
      "[Policy Evaluation]\n",
      "Finished after 27 timesteps\n",
      "Finished after 33 timesteps\n",
      "Finished after 19 timesteps\n",
      "Finished after 22 timesteps\n",
      "Finished after 29 timesteps\n",
      "Finished after 11 timesteps\n",
      "Finished after 15 timesteps\n",
      "Finished after 19 timesteps\n",
      "Finished after 27 timesteps\n",
      "Finished after 15 timesteps\n",
      "Finished after 12 timesteps\n",
      "Finished after 51 timesteps\n",
      "Finished after 13 timesteps\n",
      "Finished after 20 timesteps\n",
      "Finished after 25 timesteps\n",
      "Finished after 12 timesteps\n",
      "Finished after 24 timesteps\n",
      "Finished after 21 timesteps\n",
      "Finished after 11 timesteps\n",
      "Finished after 19 timesteps\n",
      "The test reward for episode 0 is 20.25 with sd of 9.262154177079973.\n",
      "Episode: 1\n",
      "Finished after 12 timesteps\n",
      "Episode: 2\n",
      "Finished after 16 timesteps\n",
      "Episode: 3\n",
      "Finished after 15 timesteps\n",
      "Episode: 4\n",
      "Finished after 38 timesteps\n",
      "Episode: 5\n",
      "Finished after 18 timesteps\n",
      "Episode: 6\n",
      "Finished after 13 timesteps\n",
      "Episode: 7\n",
      "Finished after 10 timesteps\n",
      "Episode: 8\n",
      "Finished after 15 timesteps\n",
      "Episode: 9\n",
      "Finished after 16 timesteps\n",
      "Episode: 10\n",
      "Finished after 26 timesteps\n",
      "Episode: 11\n",
      "Finished after 37 timesteps\n",
      "Episode: 12\n",
      "Finished after 13 timesteps\n",
      "Episode: 13\n",
      "Finished after 56 timesteps\n",
      "Episode: 14\n",
      "Finished after 16 timesteps\n",
      "Episode: 15\n",
      "Finished after 20 timesteps\n",
      "Episode: 16\n",
      "Finished after 18 timesteps\n",
      "Episode: 17\n",
      "Finished after 14 timesteps\n",
      "Episode: 18\n",
      "Finished after 86 timesteps\n",
      "Episode: 19\n",
      "Finished after 11 timesteps\n",
      "Episode: 20\n",
      "Finished after 38 timesteps\n",
      "Episode: 21\n",
      "Finished after 16 timesteps\n",
      "Episode: 22\n",
      "Finished after 12 timesteps\n",
      "Episode: 23\n",
      "Finished after 14 timesteps\n",
      "Episode: 24\n",
      "Finished after 40 timesteps\n",
      "Episode: 25\n",
      "Finished after 17 timesteps\n",
      "Episode: 26\n",
      "Finished after 42 timesteps\n",
      "Episode: 27\n",
      "Finished after 30 timesteps\n",
      "Episode: 28\n",
      "Finished after 11 timesteps\n",
      "Episode: 29\n",
      "Finished after 38 timesteps\n",
      "Episode: 30\n",
      "Finished after 15 timesteps\n",
      "Episode: 31\n",
      "Finished after 18 timesteps\n",
      "Episode: 32\n",
      "Finished after 41 timesteps\n",
      "Episode: 33\n",
      "Finished after 14 timesteps\n",
      "Episode: 34\n",
      "Finished after 39 timesteps\n",
      "Episode: 35\n",
      "Finished after 11 timesteps\n",
      "Episode: 36\n",
      "Finished after 64 timesteps\n",
      "Episode: 37\n",
      "Finished after 55 timesteps\n",
      "Episode: 38\n",
      "Finished after 20 timesteps\n",
      "Episode: 39\n",
      "Finished after 53 timesteps\n",
      "Episode: 40\n",
      "Finished after 17 timesteps\n",
      "Episode: 41\n",
      "Finished after 46 timesteps\n",
      "Episode: 42\n",
      "Finished after 22 timesteps\n",
      "Episode: 43\n",
      "Finished after 39 timesteps\n",
      "Episode: 44\n",
      "Finished after 37 timesteps\n",
      "Episode: 45\n",
      "Finished after 19 timesteps\n",
      "Episode: 46\n",
      "Finished after 52 timesteps\n",
      "Episode: 47\n",
      "Finished after 30 timesteps\n",
      "Episode: 48\n",
      "Finished after 86 timesteps\n",
      "Episode: 49\n",
      "Finished after 29 timesteps\n",
      "Episode: 50\n",
      "Finished after 19 timesteps\n",
      "Episode: 51\n",
      "Finished after 15 timesteps\n",
      "Episode: 52\n",
      "Finished after 30 timesteps\n",
      "Episode: 53\n",
      "Finished after 26 timesteps\n",
      "Episode: 54\n",
      "Finished after 18 timesteps\n",
      "Episode: 55\n",
      "Finished after 12 timesteps\n",
      "Episode: 56\n",
      "Finished after 27 timesteps\n",
      "Episode: 57\n",
      "Finished after 37 timesteps\n",
      "Episode: 58\n",
      "Finished after 17 timesteps\n",
      "Episode: 59\n",
      "Finished after 37 timesteps\n",
      "Episode: 60\n",
      "Finished after 12 timesteps\n",
      "Episode: 61\n",
      "Finished after 28 timesteps\n",
      "Episode: 62\n",
      "Finished after 31 timesteps\n",
      "Episode: 63\n",
      "Finished after 11 timesteps\n",
      "Episode: 64\n",
      "Finished after 13 timesteps\n",
      "Episode: 65\n",
      "Finished after 12 timesteps\n",
      "Episode: 66\n",
      "Finished after 21 timesteps\n",
      "Episode: 67\n",
      "Finished after 21 timesteps\n",
      "Episode: 68\n",
      "Finished after 12 timesteps\n",
      "Episode: 69\n",
      "Finished after 34 timesteps\n",
      "Episode: 70\n",
      "Finished after 12 timesteps\n",
      "Episode: 71\n",
      "Finished after 51 timesteps\n",
      "Episode: 72\n",
      "Finished after 27 timesteps\n",
      "Episode: 73\n",
      "Finished after 36 timesteps\n",
      "Episode: 74\n",
      "Finished after 17 timesteps\n",
      "Episode: 75\n",
      "Finished after 27 timesteps\n",
      "Episode: 76\n",
      "Finished after 20 timesteps\n",
      "Episode: 77\n",
      "Finished after 28 timesteps\n",
      "Episode: 78\n",
      "Finished after 22 timesteps\n",
      "Episode: 79\n",
      "Finished after 16 timesteps\n",
      "Episode: 80\n",
      "Finished after 26 timesteps\n",
      "Episode: 81\n",
      "Finished after 17 timesteps\n",
      "Episode: 82\n",
      "Finished after 12 timesteps\n",
      "Episode: 83\n",
      "Finished after 13 timesteps\n",
      "Episode: 84\n",
      "Finished after 42 timesteps\n",
      "Episode: 85\n",
      "Finished after 18 timesteps\n",
      "Episode: 86\n",
      "Finished after 30 timesteps\n",
      "Episode: 87\n",
      "Finished after 18 timesteps\n",
      "Episode: 88\n",
      "Finished after 21 timesteps\n",
      "Episode: 89\n",
      "Finished after 11 timesteps\n",
      "Episode: 90\n",
      "Finished after 12 timesteps\n",
      "Episode: 91\n",
      "Finished after 16 timesteps\n",
      "Episode: 92\n",
      "Finished after 11 timesteps\n",
      "Episode: 93\n",
      "Finished after 16 timesteps\n",
      "Episode: 94\n",
      "Finished after 44 timesteps\n",
      "Episode: 95\n",
      "Finished after 24 timesteps\n",
      "Episode: 96\n",
      "Finished after 38 timesteps\n",
      "Episode: 97\n",
      "Finished after 35 timesteps\n",
      "Episode: 98\n",
      "Finished after 22 timesteps\n",
      "Episode: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▊                                                               | 1/5 [01:05<04:22, 65.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 21 timesteps\n",
      "Episode: 0\n",
      "Finished after 49 timesteps\n",
      "[Policy Evaluation]\n",
      "Finished after 13 timesteps\n",
      "Finished after 16 timesteps\n",
      "Finished after 61 timesteps\n",
      "Finished after 17 timesteps\n",
      "Finished after 14 timesteps\n",
      "Finished after 11 timesteps\n",
      "Finished after 10 timesteps\n",
      "Finished after 27 timesteps\n",
      "Finished after 19 timesteps\n",
      "Finished after 23 timesteps\n",
      "Finished after 11 timesteps\n",
      "Finished after 31 timesteps\n",
      "Finished after 32 timesteps\n",
      "Finished after 15 timesteps\n",
      "Finished after 13 timesteps\n",
      "Finished after 19 timesteps\n",
      "Finished after 52 timesteps\n",
      "Finished after 9 timesteps\n",
      "Finished after 14 timesteps\n",
      "Finished after 13 timesteps\n",
      "The test reward for episode 0 is 20.0 with sd of 13.531444860028806.\n",
      "Episode: 1\n",
      "Finished after 13 timesteps\n",
      "Episode: 2\n",
      "Finished after 20 timesteps\n",
      "Episode: 3\n",
      "Finished after 14 timesteps\n",
      "Episode: 4\n",
      "Finished after 23 timesteps\n",
      "Episode: 5\n",
      "Finished after 29 timesteps\n",
      "Episode: 6\n",
      "Finished after 14 timesteps\n",
      "Episode: 7\n",
      "Finished after 22 timesteps\n",
      "Episode: 8\n",
      "Finished after 20 timesteps\n",
      "Episode: 9\n"
     ]
    }
   ],
   "source": [
    "#def main_a2c(args):\n",
    "# Parse command-line arguments.\n",
    "args = parse_a2c_arguments()\n",
    "env_name = args.env_name\n",
    "\n",
    "# Create the environment.\n",
    "env =  wrap_env(gym.make(env_name))\n",
    "nA = env.action_space.n\n",
    "nS = env.observation_space.shape[0]\n",
    "num_episodes = args.num_episodes\n",
    "lr = args.lr\n",
    "baseline_lr = args.baseline_lr\n",
    "critic_lr = args.critic_lr\n",
    "render = args.render\n",
    "print(f\"Configurations:{args}\")\n",
    "\n",
    "# Plot average performance of 5 trials\n",
    "num_seeds = 5\n",
    "l = num_episodes//100\n",
    "res = np.zeros((num_seeds, l))\n",
    "\n",
    "gamma = 0.99\n",
    "act_layer = torch.nn.Softmax(dim=1) \n",
    "for i in tqdm.tqdm(range(num_seeds)):\n",
    "    reward_means = []\n",
    "\n",
    "    # TODO: create networks and setup reinforce/a2c\n",
    "    history = dict.fromkeys(['train','test'],[])\n",
    "    actor = NeuralNet(input_size=nS, output_size=nA, activation=act_layer)\n",
    "    critic = NeuralNet(input_size=nS, output_size=nA, activation=act_layer)\n",
    "    A2C_net = A2C(actor=actor, actor_lr=args.lr, N=args.n, nA=nA, \n",
    "                critic=critic, critic_lr=args.critic_lr, baseline=False, a2c=False)\n",
    "    for m in range(num_episodes):\n",
    "        print(\"Episode: {}\".format(m))\n",
    "        history['train'].append(A2C_net.train(env, gamma=gamma))\n",
    "        if m % 100 == 0:\n",
    "            print(\"[Policy Evaluation]\")\n",
    "            G = np.zeros(20)   # save 20 iterations of evaluation \n",
    "            for k in range(20):\n",
    "                g = A2C_net.evaluate_policy(env)\n",
    "                G[k] = g\n",
    "            reward_mean = G.mean()\n",
    "            reward_sd = G.std()\n",
    "            print(\"The test reward for episode {0} is {1} with sd of {2}.\".format(m, reward_mean, reward_sd))\n",
    "            reward_means.append(reward_mean)\n",
    "            history['test'].append(G)\n",
    "    res[i] = np.array(reward_means)\n",
    "\n",
    "\n",
    "ks = np.arange(l)*100\n",
    "avs = np.mean(res, axis=0)\n",
    "maxs = np.max(res, axis=0)\n",
    "mins = np.min(res, axis=0)\n",
    "\n",
    "plt.fill_between(ks, mins, maxs, alpha=0.1)\n",
    "plt.plot(ks, avs, '-o', markersize=1)\n",
    "\n",
    "plt.xlabel('Episode', fontsize = 15)\n",
    "plt.ylabel('Return', fontsize = 15)\n",
    "\n",
    "if not os.path.exists('./plots'):\n",
    "    os.mkdir('./plots')\n",
    "\n",
    "if A2C_net.type == 'A2C':\n",
    "    plt.title(\"A2C Learning Curve for N = {}\".format(args.n), fontsize = 24)\n",
    "    plt.savefig(\"./plots/a2c_curve_N={}.png\".format(args.n))\n",
    "elif A2C_net.type == 'Baseline':\n",
    "    plt.title(\"Baseline Reinforce Learning Curve\".format(args.n), fontsize = 24)\n",
    "    plt.savefig(\"./plots/Baseline_Reinforce_curve.png\".format(args.n))\n",
    "else: # Reinforce\n",
    "    plt.title(\"Reinforce Learning Curve\", fontsize = 24)\n",
    "    plt.savefig(\"./plots/Reinforce_curve.png\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(env_name)\n",
    "env = gym.make(env_name) \n",
    "states = []\n",
    "states.append(np.expand_dims(env.reset(), axis=0))\n",
    "for i in range(200): # Take 200 actions\n",
    "    print(f\"Action:{i}\")\n",
    "    ac = actor(torch.Tensor(states[-1]).to(device)).squeeze(0)\n",
    "    ac_prob = ac.detach().cpu().numpy().flatten()\n",
    "    states.append(np.expand_dims(env.reset(), axis=0))\n",
    "    #\n",
    "    a_ = np.random.choice(ac_prob, p=ac_prob)   # stochastic choice\n",
    "    curr_ac = np.where(ac_prob==a_)[0][0]  # Current Action\n",
    "    s, r, terminal, _ = env.step(curr_ac)\n",
    "    # AT THIS POINT you have (state, action, reward, new_state, done)\n",
    "    #### Display Code in Jupyter Notebook ####\n",
    "    env.unwrapped.render()\n",
    "    video_recorder.capture_frame()\n",
    "    plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    ######################\n",
    "    s, r, terminal, _ = env.step(curr_ac)\n",
    "    #### Display Code for Normal Python Script ####\n",
    "    # env.render()\n",
    "    # time.sleep(0.02)\n",
    "    ######################\n",
    "    if terminal:\n",
    "        break\n",
    "\n",
    "############ Display Code ####\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xy0PNEN6C7pz"
   },
   "outputs": [],
   "source": [
    "output = actor(torch.Tensor([[ 0.39646003  ,0.7398627  ,-0.0726157  ,-0.1376104 ]]).cuda())\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:ptml]",
   "language": "python",
   "name": "conda-env-ptml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
