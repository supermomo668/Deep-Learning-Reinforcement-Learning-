{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odNaDE1zyrL2"
   },
   "source": [
    "# install dependancies, takes around 45 seconds\n",
    "\n",
    "Rendering Dependancies\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "8-AxnvAVyzQQ"
   },
   "source": [
    "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
    "#!pip install gym pyvirtualdisplay 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A-1LTSH88EE"
   },
   "source": [
    "Pacman Dependancies"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "TCelFzWY9MBI",
    "outputId": "6f32734e-4791-49d0-c7c9-1ac7853ee167"
   },
   "source": [
    "# !apt-get update > /dev/null 2>&1\n",
    "# !apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1\n",
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APXSx7hg19TH"
   },
   "source": [
    "# Imports and Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OtnCDULP74i1"
   },
   "outputs": [],
   "source": [
    "import sys, os, copy\n",
    "from pathlib import Path\n",
    "proj_folder = Path('.').absolute()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQEtc28G4niA",
    "outputId": "eb9d76c5-7e1f-441f-b2ec-70fa69fcfb98"
   },
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G9UWeToN4r7D"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env, save_path=proj_folder/'video'):\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    env = record_video.RecordVideo(env, proj_folder/'video', name_prefix='train')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3BGbWOu179M"
   },
   "source": [
    "# Run Simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ys9oSdMk7e64"
   },
   "outputs": [],
   "source": [
    "import numpy as np, torch, gym\n",
    "import torch.nn.functional as F\n",
    "from gym.wrappers import record_video\n",
    "#\n",
    "from gym import logger as gymlogger\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "global DEBUG\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cdEOSetG7h0L"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation, layers=[32,32,16]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_size, layers[0])\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(layers[0], layers[1])\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(layers[1], layers[2])\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(layers[2], output_size)\n",
    "        self.output_activation = activation\n",
    "\n",
    "        #initialize weights, following 'fan_avg' approach\n",
    "        torch.nn.init.xavier_normal_(self.linear1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear2.weight)\n",
    "        torch.nn.init.xavier_normal_(self.linear3.weight)\n",
    "        torch.nn.init.xavier_normal_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.linear1(inputs))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        x = self.output_activation(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "    def fit(self, loss, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "class A2C(object):\n",
    "    # Implementation of N-step Advantage Actor Critic.\n",
    "\n",
    "    def __init__(self, actor, actor_lr, N, nA, critic, critic_lr, baseline=False, a2c=True):\n",
    "        # Note: baseline is true if we use reinforce with baseline\n",
    "        #       a2c is true if we use a2c else reinforce\n",
    "        \n",
    "        # TODO: Initializes A2C.\n",
    "        self.type = \"A2C\" if a2c else (\"Baseline\" if baseline else \"Reinforce\")  # Pick one of: \"A2C\", \"Baseline\", \"Reinforce\"\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        actor.to(device)\n",
    "        # define meta variabless\n",
    "        self.save_dir = Path('./').absolute()/'Output'\n",
    "        if self.type == \"A2C\":\n",
    "            critic.to(device)\n",
    "            self.critic_optimizer = torch.optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "        elif self.type == \"Baseline\":\n",
    "            pass\n",
    "        else:\n",
    "            # Reinforce\n",
    "            pass\n",
    "        self.actor = actor\n",
    "        self.actor_optimizer = torch.optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "        self.N = N\n",
    "        assert self.type is not None, \"Type must be provided\"\n",
    "\n",
    "    def reinforce_criterion(self, y_pred, y_true):\n",
    "        log_y_pred = torch.log(y_pred)   # log probability of ation\n",
    "        l = torch.multiply(log_y_pred, y_true)   # (n_eps, nA)\n",
    "        l_action = torch.sum(l, axis=1)  # (n_eps,)\n",
    "        return torch.mean(l_action)\n",
    "    \n",
    "    def fit_actor(self, states, G_total, epochs=1, batch_size=1):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.actor.train()\n",
    "        n_example = len(states)\n",
    "        history = dict.fromkeys(['loss'],[])\n",
    "        for e in range(epochs):\n",
    "            for i in range(int(np.ceil(len(states)/batch_size))):\n",
    "                start_idx, end_idx = batch_size*i, min(batch_size*(i+1), n_example)\n",
    "                states_, target_ = states[start_idx: end_idx], G_total[start_idx: end_idx]\n",
    "                pred_G = self.actor(states_.cuda())\n",
    "                loss = self.reinforce_criterion(pred_G, target_.cuda())\n",
    "                # # measure metrics and record loss\n",
    "                # m1 = metric1(cls_out.cpu(), target)\n",
    "                # m2 = metric2(cls_out.cpu(), target)\n",
    "                history['loss'].append(loss)\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                if DEBUG: print(f\"Latest Loss:{history['loss'][-1]}\")\n",
    "        return history\n",
    "\n",
    "    def evaluate_policy(self, env):\n",
    "        # TODO: Compute Accumulative trajectory reward(set a trajectory length threshold if you want)\n",
    "        \"\"\" compute rewards\n",
    "        \"\"\"\n",
    "        self.actor.eval()\n",
    "        _, _, r, _ = self.generate_episode(env, render=False)\n",
    "        rtot = np.sum(np.array(r))\n",
    "        return rtot\n",
    "\n",
    "    def generate_episode(self, env, render=False):\n",
    "        \"\"\"\n",
    "\t\t# Generates an episode by executing the current policy in the given env.\n",
    "\t\t# Returns:\n",
    "\t\t# - a list of states, indexed by time step\n",
    "            shape: (t)\n",
    "\t\t# - a list of actions, indexed by time step\n",
    "            shape: (t, nA)\n",
    "\t\t# - a list of rewards, indexed by time step\n",
    "            shape: (t)\n",
    "        \"\"\"\n",
    "        states, actions, rewards, actions_probs=[], [] ,[], []\n",
    "        # \n",
    "        nS = env.observation_space.shape[0]\n",
    "        nA = env.action_space.n\n",
    "\t\t# Start episode\n",
    "        states.append(np.expand_dims(env.reset(), axis=0))\n",
    "        terminal = False\n",
    "        if render: \n",
    "            env.render(mode='rgb_array')\n",
    "        cts = 0\n",
    "        while not terminal:\n",
    "            cts+=1\n",
    "            ac = self.actor(torch.Tensor(states[-1]).to(device)).squeeze(0)   # ensure [nA] vector\n",
    "            if DEBUG: print(f\"Input state:{states[-1]}\\nOutput action:{ac}\")\n",
    "            ac_prob = ac.detach().cpu().numpy().flatten()\n",
    "            ac_prob = np.nan_to_num(ac_prob,0)\n",
    "            #\n",
    "            a_ = np.random.choice(ac_prob, p=ac_prob)   # stochastic choice\n",
    "            curr_ac = np.where(ac_prob==a_)[0][0]  # Current Action\n",
    "            action_OH = np.eye(nA)[curr_ac]  # one-hot action\n",
    "            \n",
    "            # move in direction and get environment output\n",
    "            s, r, terminal, _ = env.step(curr_ac)\n",
    "            # add to history\n",
    "            states.append(np.expand_dims(s, axis=0))\n",
    "            actions.append(action_OH)\n",
    "            actions_probs.append(ac_prob)\n",
    "            rewards.append(r)\n",
    "            curr_state = copy.deepcopy(s)\n",
    "        if DEBUG: print(f\"action probs:{ac_prob}\\nfinal action:{action_OH}\")\n",
    "        print(\"Finished after {} timesteps\".format(cts+1))\n",
    "\t\t# flatten \n",
    "        states=np.reshape(np.array(states), (-1, nS))\n",
    "        actions=np.reshape(np.array(actions), (-1, nA))\n",
    "        return np.stack(states), np.stack(actions), np.stack(rewards), np.stack(actions_probs)\n",
    "\n",
    "    def train(self, env, gamma=0.99, n=10):\n",
    "        \"\"\"\n",
    "        # Trains the model on a single episode using REINFORCE or A2C/A3C.\n",
    "        params:\n",
    "            n: number of n-steps look ahead\n",
    "        \"\"\"\n",
    "        # TODO: Implement this method. It may be helpful to call the class\n",
    "        #       method generate_episode() to generate training data.\n",
    "        eps_lim = 500    # length of episode 'limit'\n",
    "        batch_size_ratio = 1    # portion of length of states to feed in NN\n",
    "        alpha = 1e-2\n",
    "        assert 0<batch_size_ratio<=1, \"Batch size between 0 and 1\"\n",
    "        #\n",
    "        env = wrap_env(env)\n",
    "        mean_total, std_total = [], []\n",
    "        # generate episode\n",
    "        states, actions, rewards, actions_prob = self.generate_episode(env, render=True)\n",
    "        states = states[:-1]   # remove the last state\n",
    "        # get discounted reward vector\n",
    "            # [sum : r*gamma*0...r*gamma*n]\n",
    "        G_tot=[0]\n",
    "        for n_gamma, r in enumerate(reversed(rewards)):\n",
    "            # insert from last reward to beginning \n",
    "            G_tot.insert(0, r+gamma**n_gamma*G_tot[0])\n",
    "        if DEBUG: print(f\"Sum of Expected Rewards G:{G_tot}\")\n",
    "        G_tot=np.array(G_tot[:-1])\n",
    "        # weight actions by rewards and fit model\n",
    "        if DEBUG: print(f\"G_tot:[{len(G_tot)}], actions:[{len(actions)}], states:[{len(states)}]\")\n",
    "        G_total_actions = np.multiply(G_tot, actions.T).T   # (t, nA)\n",
    "        train_history = self.fit_actor(torch.Tensor(states), torch.Tensor(G_total_actions*alpha), epochs=1, batch_size=int(len(states)*batch_size_ratio))\n",
    "        return train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqrh5P-cCbe3"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xKD2PlR3WJii"
   },
   "outputs": [],
   "source": [
    "import argparse, matplotlib.pyplot as plt, tqdm\n",
    "def parse_a2c_arguments():\n",
    "    # Command-line flags are defined here.\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env-name', dest='env_name', type=str,\n",
    "                        default='CartPole-v0', help=\"Name of the environment to be run.\")   # 'LunarLander-v2'\n",
    "    parser.add_argument('--num-episodes', dest='num_episodes', type=int,\n",
    "                        default=100, help=\"Number of episodes to train on.\")\n",
    "    parser.add_argument('--lr', dest='lr', type=float,\n",
    "                        default=5e-4, help=\"The actor's learning rate.\")\n",
    "    parser.add_argument('--baseline-lr', dest='baseline_lr', type=float,\n",
    "                        default=5e-4, help=\"The actor's learning rate.\")\n",
    "    parser.add_argument('--critic-lr', dest='critic_lr', type=float,\n",
    "                        default=1e-4, help=\"The critic's learning rate.\")\n",
    "    parser.add_argument('--n', dest='n', type=int,\n",
    "                        default=100, help=\"The value of N in N-step A2C.\")\n",
    "\n",
    "    parser_group = parser.add_mutually_exclusive_group(required=False)\n",
    "    parser_group.add_argument('--render', dest='render',\n",
    "                              action='store_true',\n",
    "                              help=\"Whether to render the environment.\")\n",
    "    parser_group.add_argument('--no-render', dest='render',\n",
    "                              action='store_false',\n",
    "                              help=\"Whether to render the environment.\")\n",
    "    parser.set_defaults(render=False)\n",
    "\n",
    "    return parser.parse_known_args()[0]    #.parse_args()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Zj2YirE8nIB",
    "outputId": "e4a6e959-5eab-4606-9037-887c8f73b36f",
    "tags": []
   },
   "source": [
    "#def main_a2c(args):\n",
    "# Parse command-line arguments.\n",
    "args = parse_a2c_arguments()\n",
    "env_name = args.env_name\n",
    "\n",
    "# Create the environment.\n",
    "env =  wrap_env(gym.make(env_name))\n",
    "nA = env.action_space.n\n",
    "nS = env.observation_space.shape[0]\n",
    "num_episodes = args.num_episodes\n",
    "lr = args.lr\n",
    "baseline_lr = args.baseline_lr\n",
    "critic_lr = args.critic_lr\n",
    "render = args.render\n",
    "print(f\"Configurations:{args}\")\n",
    "\n",
    "# Plot average performance of 5 trials\n",
    "num_seeds = 5\n",
    "l = num_episodes//100\n",
    "res = np.zeros((num_seeds, l))\n",
    "\n",
    "gamma = 0.99\n",
    "act_layer = torch.nn.Softmax(dim=1) \n",
    "for i in tqdm.tqdm(range(num_seeds)):\n",
    "    reward_means = []\n",
    "\n",
    "    # TODO: create networks and setup reinforce/a2c\n",
    "    history = dict.fromkeys(['train','test'],[])\n",
    "    actor = NeuralNet(input_size=nS, output_size=nA, activation=act_layer)\n",
    "    critic = NeuralNet(input_size=nS, output_size=nA, activation=act_layer)\n",
    "    A2C_net = A2C(actor=actor, actor_lr=args.lr, N=args.n, nA=nA, \n",
    "                critic=critic, critic_lr=args.critic_lr, baseline=False, a2c=False)\n",
    "    for m in range(num_episodes):\n",
    "        print(\"Episode: {}\".format(m))\n",
    "        history['train'].append(A2C_net.train(env, gamma=gamma))\n",
    "        if m % 100 == 0:\n",
    "            print(\"[Policy Evaluation]\")\n",
    "            G = np.zeros(20)   # save 20 iterations of evaluation \n",
    "            for k in range(20):\n",
    "                g = A2C_net.evaluate_policy(env)\n",
    "                G[k] = g\n",
    "            reward_mean = G.mean()\n",
    "            reward_sd = G.std()\n",
    "            print(\"The test reward for episode {0} is {1} with sd of {2}.\".format(m, reward_mean, reward_sd))\n",
    "            reward_means.append(reward_mean)\n",
    "            history['test'].append(G)\n",
    "    res[i] = np.array(reward_means)\n",
    "\n",
    "\n",
    "ks = np.arange(l)*100\n",
    "avs = np.mean(res, axis=0)\n",
    "maxs = np.max(res, axis=0)\n",
    "mins = np.min(res, axis=0)\n",
    "\n",
    "plt.fill_between(ks, mins, maxs, alpha=0.1)\n",
    "plt.plot(ks, avs, '-o', markersize=1)\n",
    "\n",
    "plt.xlabel('Episode', fontsize = 15)\n",
    "plt.ylabel('Return', fontsize = 15)\n",
    "\n",
    "if not os.path.exists('./plots'):\n",
    "    os.mkdir('./plots')\n",
    "\n",
    "if A2C_net.type == 'A2C':\n",
    "    plt.title(\"A2C Learning Curve for N = {}\".format(args.n), fontsize = 24)\n",
    "    plt.savefig(\"./plots/a2c_curve_N={}.png\".format(args.n))\n",
    "elif A2C_net.type == 'Baseline':\n",
    "    plt.title(\"Baseline Reinforce Learning Curve\".format(args.n), fontsize = 24)\n",
    "    plt.savefig(\"./plots/Baseline_Reinforce_curve.png\".format(args.n))\n",
    "else: # Reinforce\n",
    "    plt.title(\"Reinforce Learning Curve\", fontsize = 24)\n",
    "    plt.savefig(\"./plots/Reinforce_curve.png\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_31264/1040484011.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env_name)\n",
    "env = gym.make(env_name) \n",
    "states = []\n",
    "states.append(np.expand_dims(env.reset(), axis=0))\n",
    "for i in range(200): # Take 200 actions\n",
    "    print(f\"Action:{i}\")\n",
    "    ac = actor(torch.Tensor(states[-1]).to(device)).squeeze(0)\n",
    "    ac_prob = ac.detach().cpu().numpy().flatten()\n",
    "    states.append(np.expand_dims(env.reset(), axis=0))\n",
    "    #\n",
    "    a_ = np.random.choice(ac_prob, p=ac_prob)   # stochastic choice\n",
    "    curr_ac = np.where(ac_prob==a_)[0][0]  # Current Action\n",
    "    s, r, terminal, _ = env.step(curr_ac)\n",
    "    # AT THIS POINT you have (state, action, reward, new_state, done)\n",
    "    #### Display Code in Jupyter Notebook ####\n",
    "    env.unwrapped.render()\n",
    "    video_recorder.capture_frame()\n",
    "    plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    ######################\n",
    "    s, r, terminal, _ = env.step(curr_ac)\n",
    "    #### Display Code for Normal Python Script ####\n",
    "    # env.render()\n",
    "    # time.sleep(0.02)\n",
    "    ######################\n",
    "    if terminal:\n",
    "        break\n",
    "\n",
    "############ Display Code ####\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()\n",
    "######################"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:ptml]",
   "language": "python",
   "name": "conda-env-ptml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
