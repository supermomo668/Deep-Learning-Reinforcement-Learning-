{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odNaDE1zyrL2"
   },
   "source": [
    "# install dependancies, takes around 45 seconds\n",
    "\n",
    "Rendering Dependancies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-AxnvAVyzQQ"
   },
   "outputs": [],
   "source": [
    "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
    "#!pip install gym pyvirtualdisplay 2>&1\n",
    "#!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A-1LTSH88EE"
   },
   "source": [
    "Pacman Dependancies"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "TCelFzWY9MBI",
    "outputId": "6f32734e-4791-49d0-c7c9-1ac7853ee167"
   },
   "source": [
    "# !apt-get update > /dev/null 2>&1\n",
    "# !apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1\n",
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQEtc28G4niA",
    "outputId": "eb9d76c5-7e1f-441f-b2ec-70fa69fcfb98"
   },
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APXSx7hg19TH",
    "tags": []
   },
   "source": [
    "# Imports and Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OtnCDULP74i1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys, os, copy\n",
    "from pathlib import Path\n",
    "from a2c.a2c import *\n",
    "#from torch.distributions import Categorical\n",
    "proj_folder = Path('.').absolute()\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "action_dist = Categorical(NeuralNet(nS, nA, activation=nn.Softmax(dim=1))(torch.randn((1,4))))\n",
    "#\n",
    "action = action_dist.sample()\n",
    "action_logprob = action_dist.log_prob(action)\n",
    "print(action, action_logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3BGbWOu179M"
   },
   "source": [
    "# Run Simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "xKD2PlR3WJii"
   },
   "outputs": [],
   "source": [
    "import argparse, matplotlib.pyplot as plt, tqdm\n",
    "def parse_a2c_arguments():\n",
    "    # Command-line flags are defined here.\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env-name', dest='env_name', type=str,\n",
    "                        default='CartPole-v0', help=\"Name of the environment to be run.\")   # 'LunarLander-v2'\n",
    "    parser.add_argument('--num-episodes', dest='num_episodes', type=int,\n",
    "                        default=3500, help=\"Number of episodes to train on.\")    # 3500\n",
    "    parser.add_argument('--lr', dest='lr', type=float,\n",
    "                        default=5e-4, help=\"The actor's learning rate.\")\n",
    "    parser.add_argument('--use_a2c', dest='use_a2c', type=bool,\n",
    "                        default=True, help=\"Use A2C\")\n",
    "    parser.add_argument('--use_baseline', dest='use_baseline', type=bool,\n",
    "                        default=True, help=\"Use baseline model\")\n",
    "    parser.add_argument('--baseline-lr', dest='baseline_lr', type=float,\n",
    "                        default=5e-4, help=\"The actor's learning rate.\")\n",
    "    parser.add_argument('--critic-lr', dest='critic_lr', type=float,\n",
    "                        default=1e-4, help=\"The critic's learning rate.\")\n",
    "    parser.add_argument('--n', dest='n', type=int,\n",
    "                        default=100, help=\"The value of N in N-step A2C.\")\n",
    "\n",
    "    parser_group = parser.add_mutually_exclusive_group(required=False)\n",
    "    parser_group.add_argument('--render', dest='render',\n",
    "                              action='store_true',\n",
    "                              help=\"Whether to render the environment.\")\n",
    "    parser_group.add_argument('--no-render', dest='render',\n",
    "                              action='store_false',\n",
    "                              help=\"Whether to render the environment.\")\n",
    "    parser.set_defaults(render=False)\n",
    "\n",
    "    return parser.parse_known_args()[0]    #.parse_args()\n",
    "args = parse_a2c_arguments()\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations:Namespace(baseline_lr=0.0005, critic_lr=0.0001, env_name='CartPole-v0', lr=0.0005, n=100, num_episodes=3500, render=False, use_a2c=True, use_baseline=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                           | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:0\n",
      "Run type:A2C|Device:cuda\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 0 is 25.1 with sd of 15.610573339887294.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 100 is 21.9 with sd of 11.099099062536562.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 200 is 24.55 with sd of 11.573569026017859.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 300 is 22.4 with sd of 13.093509842666327.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 400 is 28.0 with sd of 14.720733677368122.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 500 is 25.3 with sd of 13.255564869140809.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 600 is 23.45 with sd of 7.513155129504515.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 700 is 29.5 with sd of 19.971229306179428.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 800 is 42.8 with sd of 28.4035209085071.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 900 is 56.15 with sd of 33.835299614455906.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1000 is 109.75 with sd of 58.62412046248541.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1100 is 107.25 with sd of 41.554632714054875.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1200 is 165.1 with sd of 49.89078071147013.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1300 is 156.4 with sd of 33.63539802053783.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1400 is 185.15 with sd of 27.88776613499188.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1500 is 180.6 with sd of 23.9131762842162.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1600 is 194.05 with sd of 17.608165719347376.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1700 is 184.3 with sd of 21.638160735145675.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1800 is 199.65 with sd of 1.5256146302392357.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1900 is 166.05 with sd of 39.368102570482115.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2000 is 200.0 with sd of 0.0.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2100 is 178.35 with sd of 24.519940864529016.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2200 is 177.9 with sd of 23.54336424557884.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2300 is 200.0 with sd of 0.0.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2400 is 199.6 with sd of 1.7435595774162693.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2500 is 198.05 with sd of 5.903177110675234.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2600 is 164.45 with sd of 32.38128317408067.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2700 is 172.8 with sd of 35.75695736496605.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2800 is 200.0 with sd of 0.0.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2900 is 200.0 with sd of 0.0.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3000 is 200.0 with sd of 0.0.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3100 is 199.9 with sd of 0.43588989435406733.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3200 is 199.0 with sd of 4.358898943540674.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                           | 0/5 [53:22<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 3.51 GiB already allocated; 0 bytes free; 3.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13364/2616199675.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA2C_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain_a2c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Notes\\CMU\\10703 - Deep RL\\F22_10703_Homework_2\\hw2_code\\pytorch\\a2c\\a2c.py\u001b[0m in \u001b[0;36mmain_a2c\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    226\u001b[0m                       critic=critic, critic_lr=args.critic_lr, baseline=args.use_baseline, a2c=args.use_a2c)\n\u001b[0;32m    227\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m             \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA2C_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0meval_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[Policy Evaluation]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Notes\\CMU\\10703 - Deep RL\\F22_10703_Homework_2\\hw2_code\\pytorch\\a2c\\a2c.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, env, gamma, n)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mmean_total\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;31m# generate episode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_logprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# remove the last terminal state  # (t, 1, 4) -> (t,4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# Reinforce / baseline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Notes\\CMU\\10703 - Deep RL\\F22_10703_Homework_2\\hw2_code\\pytorch\\a2c\\a2c.py\u001b[0m in \u001b[0;36mgenerate_episode\u001b[1;34m(self, env, render)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Input state:{states[-1]}\\nOutput action:{ac}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;31m# cateogircal distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0maction_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0mcurr_ac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Current Action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mac_logprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_ac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda\\Anaconda3\\envs\\ptml\\lib\\site-packages\\torch\\distributions\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`probs` parameter must be at least one-dimensional.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 3.51 GiB already allocated; 0 bytes free; 3.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "history, res, A2C_net = main_a2c(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ks = np.arange(args.num_episodes/100)*100\n",
    "avs = np.mean(res, axis=0)\n",
    "maxs = np.max(res, axis=0)\n",
    "mins = np.min(res, axis=0)\n",
    "\n",
    "plt.fill_between(ks, mins, maxs, alpha=0.1)\n",
    "plt.plot(ks, avs, '-o', markersize=1)\n",
    "\n",
    "plt.xlabel('Episode', fontsize = 15)\n",
    "plt.ylabel('Return', fontsize = 15)\n",
    "\n",
    "if not os.path.exists('./plots'):\n",
    "    os.mkdir('./plots')\n",
    "print(A2C_net.type, args.n)\n",
    "if A2C_net.type == 'A2C' or A2C_net.type == 2:\n",
    "    plt.title(\"A2C Learning Curve for N = {}\".format(args.n), fontsize = 24)\n",
    "    plt.savefig(\"./plots/a2c_curve_N={}.png\".format(args.n))\n",
    "elif A2C_net.type == 'Baseline' or A2C_net.type == 1:\n",
    "    plt.title(\"Baseline Reinforce Learning Curve\".format(args.n), fontsize = 24)\n",
    "    plt.savefig(\"./plots/Baseline_Reinforce_curve.png\".format(args.n))\n",
    "elif A2C_net.type == 'Reinforce' or A2C_net.type == 0: # Reinforce\n",
    "    plt.title(\"Reinforce Learning Curve\", fontsize = 24)\n",
    "    plt.savefig(\"./plots/Reinforce_curve(2).png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:ptml]",
   "language": "python",
   "name": "conda-env-ptml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
