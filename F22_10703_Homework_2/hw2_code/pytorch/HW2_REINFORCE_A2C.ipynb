{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odNaDE1zyrL2"
   },
   "source": [
    "# install dependancies, takes around 45 seconds\n",
    "\n",
    "Rendering Dependancies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8-AxnvAVyzQQ"
   },
   "outputs": [],
   "source": [
    "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
    "#!pip install gym pyvirtualdisplay 2>&1\n",
    "#!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A-1LTSH88EE"
   },
   "source": [
    "Pacman Dependancies"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "TCelFzWY9MBI",
    "outputId": "6f32734e-4791-49d0-c7c9-1ac7853ee167"
   },
   "source": [
    "# !apt-get update > /dev/null 2>&1\n",
    "# !apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1\n",
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQEtc28G4niA",
    "outputId": "eb9d76c5-7e1f-441f-b2ec-70fa69fcfb98"
   },
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APXSx7hg19TH",
    "tags": []
   },
   "source": [
    "# Imports and Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OtnCDULP74i1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:cuda\n"
     ]
    }
   ],
   "source": [
    "import sys, os, copy\n",
    "from pathlib import Path\n",
    "from a2c.a2c import *\n",
    "#from torch.distributions import Categorical\n",
    "proj_folder = Path('.').absolute()\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "action_dist = Categorical(NeuralNet(nS, nA, activation=nn.Softmax(dim=1))(torch.randn((1,4))))\n",
    "#\n",
    "action = action_dist.sample()\n",
    "action_logprob = action_dist.log_prob(action)\n",
    "print(action, action_logprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3BGbWOu179M"
   },
   "source": [
    "# Run Simulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2c.net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xKD2PlR3WJii"
   },
   "outputs": [],
   "source": [
    "import argparse, matplotlib.pyplot as plt, tqdm\n",
    "def parse_a2c_arguments():\n",
    "    # Command-line flags are defined here.\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env-name', dest='env_name', type=str,\n",
    "                        default='CartPole-v0', help=\"Name of the environment to be run.\")   # 'LunarLander-v2'\n",
    "    parser.add_argument('--num-episodes', dest='num_episodes', type=int,\n",
    "                        default=10, help=\"Number of episodes to train on.\")    # 3500\n",
    "    parser.add_argument('--lr', dest='lr', type=float,\n",
    "                        default=5e-4, help=\"The actor's learning rate.\")\n",
    "    parser.add_argument('--use_a2c', dest='use_a2c', type=bool,\n",
    "                        default=False, help=\"Use A2C\")\n",
    "    parser.add_argument('--use_baseline', dest='use_baseline', type=bool,\n",
    "                        default=True, help=\"Use baseline model\")\n",
    "    parser.add_argument('--baseline-lr', dest='baseline_lr', type=float,\n",
    "                        default=5e-4, help=\"The actor's learning rate.\")\n",
    "    parser.add_argument('--critic-lr', dest='critic_lr', type=float,\n",
    "                        default=1e-4, help=\"The critic's learning rate.\")\n",
    "    parser.add_argument('--n', dest='n', type=int,\n",
    "                        default=1, help=\"The value of N in N-step A2C.\")\n",
    "    parser.add_argument('--shared_backbone', dest='shared_backbone', type=bool,\n",
    "                        default=True, help=\"Sharing backbone between critic/actor\")\n",
    "\n",
    "    parser_group = parser.add_mutually_exclusive_group(required=False)\n",
    "    parser_group.add_argument('--render', dest='render',\n",
    "                              action='store_true',\n",
    "                              help=\"Whether to render the environment.\")\n",
    "    parser_group.add_argument('--no-render', dest='render',\n",
    "                              action='store_false',\n",
    "                              help=\"Whether to render the environment.\")\n",
    "    parser.set_defaults(render=False)\n",
    "\n",
    "    return parser.parse_known_args()[0]    #.parse_args()\n",
    "args = parse_a2c_arguments()\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations:Namespace(baseline_lr=0.0005, critic_lr=0.0001, env_name='CartPole-v0', lr=0.0005, n=1, num_episodes=3500, render=False, use_a2c=False, use_baseline=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                               | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:0\n",
      "Run type:Baseline|Device:cuda\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 0 is 24.6 with sd of 12.105370708904376.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 100 is 15.3 with sd of 6.108191221630181.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 200 is 11.6 with sd of 3.903844259188627.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 300 is 10.25 with sd of 1.0428326807307104.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 400 is 9.85 with sd of 0.9096702699330127.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 500 is 10.9 with sd of 2.2561028345356955.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 600 is 23.2 with sd of 14.15132502630054.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 700 is 20.75 with sd of 14.672678692045293.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 800 is 26.1 with sd of 16.513327950476853.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 900 is 23.6 with sd of 16.6715326230074.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1000 is 23.1 with sd of 11.366177897604805.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1100 is 20.05 with sd of 10.02235002382176.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1200 is 14.75 with sd of 5.078139423056441.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1300 is 33.85 with sd of 25.06446688042656.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1400 is 19.3 with sd of 8.73555951270438.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1500 is 22.95 with sd of 11.633894446830777.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1600 is 30.95 with sd of 14.904613379755947.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1700 is 22.9 with sd of 13.619471355379401.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1800 is 24.1 with sd of 13.175355782672437.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1900 is 22.15 with sd of 10.992156294376459.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2000 is 23.5 with sd of 11.70256382165891.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2100 is 21.65 with sd of 11.992810346203262.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2200 is 32.95 with sd of 22.502166562355725.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2300 is 26.9 with sd of 14.52205219657332.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2400 is 21.05 with sd of 7.774799032772487.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2500 is 24.9 with sd of 11.139569111954016.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2600 is 27.95 with sd of 15.02822344789962.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2700 is 33.8 with sd of 18.911901014969384.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2800 is 51.25 with sd of 33.688091367722215.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2900 is 49.05 with sd of 25.38597053492342.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3000 is 47.55 with sd of 24.719374992098807.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3100 is 55.8 with sd of 29.116318448595116.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3200 is 39.65 with sd of 24.368576076578623.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3300 is 55.1 with sd of 37.429800961266146.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3400 is 42.25 with sd of 27.63489641739227.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████▊                                       | 1/5 [1:06:53<4:27:32, 4013.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:1\n",
      "Run type:Baseline|Device:cuda\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 0 is 17.35 with sd of 7.970414041942864.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 100 is 18.85 with sd of 9.639891078222824.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 200 is 24.75 with sd of 19.03647814066457.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 300 is 26.65 with sd of 14.646757320308136.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 400 is 21.45 with sd of 10.170914413168562.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 500 is 22.75 with sd of 10.96300597464035.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 600 is 19.15 with sd of 6.101434257615172.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 700 is 19.0 with sd of 6.985699678629192.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 800 is 23.35 with sd of 10.603183484218311.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 900 is 21.8 with sd of 11.595688854052613.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1000 is 26.4 with sd of 15.298365925810508.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1100 is 21.25 with sd of 8.537417642355328.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1200 is 20.45 with sd of 10.200367640433358.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1300 is 23.85 with sd of 14.80970965279198.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1400 is 18.1 with sd of 5.166236541235795.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1500 is 22.8 with sd of 14.627371602581237.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1600 is 43.7 with sd of 29.151500818997295.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1700 is 31.35 with sd of 17.22868248009696.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1800 is 35.4 with sd of 27.13005713226568.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1900 is 21.15 with sd of 9.613922196481516.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2000 is 23.8 with sd of 14.698979556418195.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2100 is 23.2 with sd of 10.81942697188719.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2200 is 36.6 with sd of 22.929020912372163.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2300 is 19.85 with sd of 7.8757539321642085.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2400 is 28.95 with sd of 17.49707118348668.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2500 is 19.2 with sd of 9.744742172063868.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2600 is 22.85 with sd of 12.26896491151556.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2700 is 41.6 with sd of 22.867881406024477.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2800 is 23.45 with sd of 9.733832749744574.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2900 is 22.75 with sd of 12.185544714948119.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3000 is 47.35 with sd of 33.03827325996321.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3100 is 22.15 with sd of 10.687726605784787.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3200 is 44.75 with sd of 33.08606202013168.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3300 is 27.85 with sd of 17.126806474062818.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3400 is 26.75 with sd of 18.22326809329216.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████▌                             | 2/5 [2:14:09<3:21:19, 4026.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:2\n",
      "Run type:Baseline|Device:cuda\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 0 is 22.5 with sd of 13.384692749555366.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 100 is 20.05 with sd of 9.896842930955305.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 200 is 14.8 with sd of 3.54400902933387.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 300 is 22.3 with sd of 12.95414991421668.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 400 is 17.15 with sd of 5.3969899017878475.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 500 is 16.3 with sd of 8.167618992093105.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 600 is 18.5 with sd of 6.7712628068920795.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 700 is 20.95 with sd of 14.524031809384057.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 800 is 23.75 with sd of 10.779030568655049.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 900 is 26.35 with sd of 13.073924429948336.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1000 is 26.8 with sd of 17.186040847152668.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1100 is 20.65 with sd of 11.4073441256061.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1200 is 19.1 with sd of 5.98247440445841.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1300 is 22.8 with sd of 7.909487973314076.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1400 is 23.25 with sd of 12.879732140071857.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1500 is 27.6 with sd of 22.319050159000945.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1600 is 22.75 with sd of 8.642193008721803.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1700 is 24.9 with sd of 16.908281994336384.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1800 is 25.15 with sd of 11.83332159623831.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 1900 is 23.0 with sd of 9.705668446840743.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2000 is 24.8 with sd of 12.412090879461044.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2100 is 22.1 with sd of 11.844407963254222.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2200 is 23.05 with sd of 8.963676700997198.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2300 is 25.9 with sd of 14.117011015083895.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2400 is 24.05 with sd of 12.862639697978018.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2500 is 22.35 with sd of 12.877402688430614.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2600 is 28.85 with sd of 16.475056904302335.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2700 is 25.1 with sd of 13.14115672229808.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2800 is 21.45 with sd of 10.67461943115538.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 2900 is 23.7 with sd of 12.598015716770636.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3000 is 22.7 with sd of 12.251938622112013.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3100 is 22.7 with sd of 14.608559134972893.\n",
      "[Policy Evaluation]\n",
      "The test reward for episode 3200 is 17.0 with sd of 10.94988584415381.\n"
     ]
    }
   ],
   "source": [
    "history, res, A2C_net = main_a2c(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ks = np.arange(args.num_episodes/100)*100\n",
    "avs = np.mean(res, axis=0)\n",
    "maxs = np.max(res, axis=0)\n",
    "mins = np.min(res, axis=0)\n",
    "\n",
    "plt.fill_between(ks, mins, maxs, alpha=0.1)\n",
    "plt.plot(ks, avs, '-o', markersize=1)\n",
    "\n",
    "plt.xlabel('Episode', fontsize = 15)\n",
    "plt.ylabel('Return', fontsize = 15)\n",
    "\n",
    "if not os.path.exists('./plots'):\n",
    "    os.mkdir('./plots')\n",
    "print(A2C_net.type, args.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if A2C_net.type == 'A2C' or A2C_net.type == 2:\n",
    "    plt.title(\"A2C Learning Curve for N = {}\".format(args.n), fontsize = 24)\n",
    "    plt.savefig(\"./plots/a2c_curve_N={}.png\".format(args.n))\n",
    "elif A2C_net.type == 'Baseline' or A2C_net.type == 1:\n",
    "    plt.title(\"Baseline Reinforce Learning Curve\".format(args.n), fontsize = 24)\n",
    "    plt.savefig(\"./plots/Baseline_Reinforce_curve.png\".format(args.n))\n",
    "elif A2C_net.type == 'Reinforce' or A2C_net.type == 0: # Reinforce\n",
    "    plt.title(\"Reinforce Learning Curve\", fontsize = 24)\n",
    "    plt.savefig(\"./plots/Reinforce_curve(2).png\") "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:ptml]",
   "language": "python",
   "name": "conda-env-ptml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
