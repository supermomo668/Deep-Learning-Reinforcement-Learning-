{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "d6c05b54-ecbf-49f9-b564-6010438db6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.regularizers import L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5631d4a6-b88a-4791-960a-649c1f17e3f4",
   "metadata": {},
   "source": [
    "Function Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "d338b14d-e0da-4aec-8b07-7af238bb65f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "from game import *\n",
    "from main import *\n",
    "from mcts import *\n",
    "from self_play import *\n",
    "from networks import *\n",
    "from networks_base import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a530bf8-765c-483f-b2bc-35cf9eaea522",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "94a08b24-ba31-4233-a2cb-652cb66e8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = CartPoleNetwork(\n",
    "    action_size=2, state_shape=(None, 4), embedding_size=4, max_value=200)\n",
    "config = get_cartpole_config(50)  # Create Environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Create buffer to store games\n",
    "replay_buffer = ReplayBuffer(config)\n",
    "#self_play(env, config, replay_buffer, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "ae7dbcdd-5df3-4421-a94f-72c61cccd2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_space_size': 2, 'games_per_epoch': 2, 'num_epochs': 50, 'train_per_epoch': 30, 'episodes_per_test': 10, 'visit_softmax_temperature_fn': <function visit_softmax_temperature at 0x000001D7A0E6E3A0>, 'max_moves': 200, 'num_simulations': 50, 'discount': 0.997, 'root_dirichlet_alpha': 0.1, 'root_exploration_fraction': 0.25, 'pb_c_base': 19652, 'pb_c_init': 1.25, 'known_bounds': None, 'buffer_size': 200, 'batch_size': 512, 'num_unroll_steps': 5, 'td_steps': 10, 'lr_init': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print(config.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "0c6d2d91-ff2c-444f-9d8e-355e9c3c10c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 [0.5100251 0.4899749]\n"
     ]
    }
   ],
   "source": [
    "print(config.root_exploration_fraction, np.random.dirichlet([config.root_dirichlet_alpha]*2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22297d-b1cb-4573-9082-ab3032d91ae2",
   "metadata": {},
   "source": [
    "### Self_play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "957b4032-bb95-4475-a940-c7c4a0a22ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "min_max_stats= MinMaxStats(config.known_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "4cc42cc8-5289-44de-98e6-c4470967c268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00363736, -0.02077954,  0.00602416,  0.04816066], dtype=float32)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_state = env.reset()\n",
    "# Create Game Objects\n",
    "game = Game(config.action_space_size, config.discount, start_state)\n",
    "root = Node(0)\n",
    "game.curr_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607174d5-04eb-4e2f-bdf6-ac75039dd0bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "2153cb58-d812-4a0d-b0c4-8705a5040b01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "root = Node(0)\n",
    "value = expand_root(root, list(range(config.action_space_size)),\n",
    "                    network, current_state=game.curr_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "b17199bb-4ddd-42f7-8b99-22644ad1513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backpropagate([root], value, config.discount, min_max_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "293cebf3-1389-454f-9736-b33ad6e68327",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_exploration_noise(config, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "866eb783-6901-4376-a386-ca964ef1b855",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History:[1], 0\n",
      "70.53214201288333\n",
      "History:[1, 1], 0\n",
      "70.21464903408993\n"
     ]
    }
   ],
   "source": [
    "def run_mcts(config, root, network, min_max_stats):\n",
    "    \"\"\"\n",
    "    Main loop for MCTS for config.num_simulations simulations\n",
    "\n",
    "    root: the root node\n",
    "    network: the network\n",
    "    min_max_stats: the min max stats object for the simulation\n",
    "\n",
    "    Hint:\n",
    "    The MCTS should capture selection, expansion and backpropagation\n",
    "    \"\"\"\n",
    "    for i in range(2):\n",
    "        history = []\n",
    "        node = root\n",
    "        search_path = [node]  #  node object\n",
    "\n",
    "        while node.expanded:\n",
    "            action, node = select_child(config, node, min_max_stats)\n",
    "            history.append(action)\n",
    "            search_path.append(node)\n",
    "        print(f\"History:{history}, {node.value_sum}\")\n",
    "        parent = search_path[-2]\n",
    "        action = history[-1]\n",
    "        value = expand_node(node, list(range(config.action_space_size)),\n",
    "                            network, parent.hidden_representation, action)\n",
    "        \n",
    "        backpropagate(search_path, value,\n",
    "                      config.discount, min_max_stats)\n",
    "        print(f\"{node.value_sum}\")\n",
    "        ##\n",
    "run_mcts(config, root, network, min_max_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "28ca1df5-ada0-4a60-ab27-d9005c406b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select action| visit counts:[(0, 0), (2, 1)]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "games_played = 0\n",
    "action = select_action(config, games_played, root, network)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1667b-d8ac-4ed3-9cca-b72a20435cd3",
   "metadata": {},
   "source": [
    "### self-Play, network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "77e4e3fd-11c4-4a37-8a7e-4ae36fcfbff6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select action| visit counts:[(5, 0), (45, 1)]\n",
      "Select action| visit counts:[(7, 0), (43, 1)]\n",
      "Select action| visit counts:[(4, 0), (46, 1)]\n",
      "Select action| visit counts:[(27, 0), (23, 1)]\n",
      "Select action| visit counts:[(4, 0), (46, 1)]\n",
      "Select action| visit counts:[(4, 0), (46, 1)]\n",
      "Select action| visit counts:[(4, 0), (46, 1)]\n",
      "Select action| visit counts:[(4, 0), (46, 1)]\n",
      "Select action| visit counts:[(4, 0), (46, 1)]\n",
      "Total reward for game: 9.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "returns = 0\n",
    "game = play_game(config, network, env, games_played)\n",
    "replay_buffer.save_game(game)\n",
    "returns += sum(game.reward_history)\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "bffc9f2b-1905-467f-b741-e9e57e64f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 16) (512, 16)\n"
     ]
    }
   ],
   "source": [
    "batch = replay_buffer.sample_batch()\n",
    "(state_batch, targets_init_batch, targets_recurrent_batch, actions_batch) = batch\n",
    "h, v, pi_logits = network.initial_model(np.stack(state_batch))\n",
    "target_value_batch, _, target_policy_batch = zip(*targets_init_batch)\n",
    "target_value_batch = network._scalar_to_support(\n",
    "            tf.convert_to_tensor(target_value_batch))\n",
    "print(v.shape, target_value_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "98803415-e3b3-45db-a763-3eabef067620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> <class 'tuple'> <class 'tuple'>\n",
      "512 512 512 512\n"
     ]
    }
   ],
   "source": [
    "for actions_batch_, targets_batch_ in zip(actions_batch, targets_recurrent_batch):\n",
    "    target_value_batch, target_reward_batch, target_policy_batch = zip(\n",
    "                *targets_batch_)\n",
    "    print(type(target_value_batch), type(target_reward_batch), type(target_policy_batch))\n",
    "    break\n",
    "target_value_batch = tf.convert_to_tensor(target_value_batch)\n",
    "target_value_batch = network._scalar_to_support(target_value_batch)\n",
    "\n",
    "target_policy_batch = tf.convert_to_tensor(target_policy_batch)\n",
    "target_reward_batch = tf.convert_to_tensor(target_reward_batch)\n",
    "print(len(actions_batch_), len(target_value_batch), len(target_reward_batch), len(target_policy_batch))\n",
    "\n",
    "# game.state_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "cc54c7a8-bc4e-4dd5-a6df-4f3756dab4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.1814256, shape=(), dtype=float32) tf.Tensor(0.7892106, shape=(), dtype=float32) tf.Tensor(0.6429982, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "regress_critierion = tf.keras.losses.MeanSquaredError()\n",
    "ce_criterion = tf.nn.softmax_cross_entropy_with_logits\n",
    "# loss\n",
    "v_loss = tf.math.reduce_mean(scale_gradient(ce_criterion(target_value_batch, v), 1/4))\n",
    "r_loss = regress_critierion(target_reward_batch, r)\n",
    "pi_loss = tf.math.reduce_mean(ce_criterion(target_policy_batch, pi_logits))\n",
    "print(v_loss, r_loss, pi_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "830a054c-5385-4f1c-ab9a-cbcff2510c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#network._conditioned_hidden_state(h[0:1], actions_batch_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "c0d8ad3c-f85b-43e4-8232-44521404d5ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "h_con = np.apply_along_axis(lambda x: tf.squeeze(network._conditioned_hidden_state(np.expand_dims(x[:-1], 0), x[-1]), 0), \n",
    "                    axis=1, arr=tf.concat((h, np.expand_dims(np.array(actions_batch_),1)), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "db93cca2-1033-46c5-b954-bbfa555a6e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 4) (512, 1) (512, 16) (512, 2)\n"
     ]
    }
   ],
   "source": [
    "h, r, v, pi_logits = network.recurrent_model(h_con)\n",
    "print(h.shape, r.shape, v.shape, pi_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "5c8aae7d-260d-4fa3-ad9f-145fc9c4f35c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:7.308935642242432\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dense_20/kernel:0', 'dense_20/bias:0', 'dense_21/kernel:0', 'dense_21/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    }
   ],
   "source": [
    "train_results = TrainResults()\n",
    "optimizer = Adam(learning_rate=config.lr_init)\n",
    "update_weights(config, network, optimizer, replay_buffer.sample_batch(), train_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d302e-998a-4c9b-aeca-da183c1c22f3",
   "metadata": {},
   "source": [
    "### Self-play full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "7ead001d-0a7e-4808-a6ca-359aab692a76",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number 0\n",
      "Select action| visit counts:[(37, 0), (13, 1)]\n",
      "Select action| visit counts:[(9, 0), (41, 1)]\n",
      "Select action| visit counts:[(9, 0), (41, 1)]\n",
      "Select action| visit counts:[(37, 0), (13, 1)]\n",
      "Select action| visit counts:[(9, 0), (41, 1)]\n",
      "Select action| visit counts:[(9, 0), (41, 1)]\n",
      "Select action| visit counts:[(9, 0), (41, 1)]\n",
      "Select action| visit counts:[(13, 0), (37, 1)]\n",
      "Select action| visit counts:[(35, 0), (15, 1)]\n",
      "Select action| visit counts:[(7, 0), (43, 1)]\n",
      "Select action| visit counts:[(7, 0), (43, 1)]\n",
      "Select action| visit counts:[(34, 0), (16, 1)]\n",
      "Select action| visit counts:[(35, 0), (15, 1)]\n",
      "Select action| visit counts:[(7, 0), (43, 1)]\n",
      "Select action| visit counts:[(7, 0), (43, 1)]\n",
      "Total reward for game: 15.0\n",
      "Train score: 41.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for axis 1 with size 16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [298], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     14\u001b[0m     batch \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample_batch()\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mupdate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Notes\\CMU\\10703 - Deep RL\\F22_10703_HW5\\muzero_code\\networks.py:279\u001b[0m, in \u001b[0;36mupdate_weights\u001b[1;34m(config, network, optimizer, batch, train_results)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_results\u001b[38;5;241m.\u001b[39mtotal_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m--> 279\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcb_get_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m network\u001b[38;5;241m.\u001b[39mtrain_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda\\Anaconda3\\envs\\ptml\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:537\u001b[0m, in \u001b[0;36mOptimizerV2.minimize\u001b[1;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, var_list, grad_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    507\u001b[0m   \u001b[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \n\u001b[0;32m    509\u001b[0m \u001b[38;5;124;03m  This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    535\u001b[0m \n\u001b[0;32m    536\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 537\u001b[0m   grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    539\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda\\Anaconda3\\envs\\ptml\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:581\u001b[0m, in \u001b[0;36mOptimizerV2._compute_gradients\u001b[1;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(var_list):\n\u001b[0;32m    580\u001b[0m   tape\u001b[38;5;241m.\u001b[39mwatch(var_list)\n\u001b[1;32m--> 581\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(var_list):\n\u001b[0;32m    583\u001b[0m   var_list \u001b[38;5;241m=\u001b[39m var_list()\n",
      "File \u001b[1;32m~\\OneDrive\\Notes\\CMU\\10703 - Deep RL\\F22_10703_HW5\\muzero_code\\networks.py:227\u001b[0m, in \u001b[0;36mupdate_weights.<locals>.loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m    223\u001b[0m target_value_batch, _, target_policy_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtargets_init_batch)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Use this to convert scalar value targets to categorical representation\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# This now matches output of initial_model, and can be used with cross entropy loss\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m target_value_batch \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scalar_to_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_value_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE: Compute the loss of the first pass (no reward loss)\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# Remember to scale value loss!\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;66;03m# scale the gradient at the start of the dynamics function by 1/2.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m h \u001b[38;5;241m=\u001b[39m scale_gradient(h, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Notes\\CMU\\10703 - Deep RL\\F22_10703_HW5\\muzero_code\\networks.py:95\u001b[0m, in \u001b[0;36mCartPoleNetwork._scalar_to_support\u001b[1;34m(self, target_value)\u001b[0m\n\u001b[0;32m     93\u001b[0m floor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mfloor(target_value)\n\u001b[0;32m     94\u001b[0m rest \u001b[38;5;241m=\u001b[39m target_value \u001b[38;5;241m-\u001b[39m floor\n\u001b[1;32m---> 95\u001b[0m \u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfloor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m rest\n\u001b[0;32m     96\u001b[0m indexes \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(floor, tf\u001b[38;5;241m.\u001b[39mint32) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     97\u001b[0m mask \u001b[38;5;241m=\u001b[39m indexes \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_support_size\n",
      "\u001b[1;31mIndexError\u001b[0m: index 16 is out of bounds for axis 1 with size 16"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=config.lr_init)\n",
    "games_played = 0\n",
    "test_rewards = TestResults()\n",
    "train_results = TrainResults()\n",
    "for i in range(1):  # Number of Steps of train/play alternations\n",
    "    print(f\"Epoch Number {i}\")\n",
    "    for _ in range(1):\n",
    "        game = play_game(config, network, env, games_played)\n",
    "        replay_buffer.save_game(game)\n",
    "        returns += sum(game.reward_history)\n",
    "        games_played += 1\n",
    "    print(\"Train score:\", returns/1)\n",
    "    for _ in range(1):\n",
    "        batch = replay_buffer.sample_batch()\n",
    "        update_weights(config, network, optimizer, batch, train_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptml",
   "language": "python",
   "name": "ptml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
