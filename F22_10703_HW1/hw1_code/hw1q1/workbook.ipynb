{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36290b38-4326-48b7-b152-d0c048938e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge black\n",
    "# !pip install nb_black\n",
    "# !conda install -c conda-forge jupyterlab_code_formatter\n",
    "# %load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e6ac205-7c9f-4fd4-a665-2ff2ef3eda66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from __future__ import division, absolute_import\n",
    "from __future__ import print_function, unicode_literals\n",
    "\n",
    "import numpy as np, seaborn as sns, matplotlib.pyplot as plt, random\n",
    "from pathlib import Path\n",
    "import gym, lake_envs as lake_env\n",
    "\n",
    "\n",
    "def print_policy(policy, action_names):\n",
    "    \"\"\"Print the policy in human-readable format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy: np.ndarray\n",
    "      Array of state to action number mappings\n",
    "    action_names: dict\n",
    "      Mapping of action numbers to characters representing the action.\n",
    "    \"\"\"\n",
    "    str_policy = policy.astype('str')\n",
    "    for action_num, action_name in action_names.items():\n",
    "        np.place(str_policy, policy == action_num, action_name)\n",
    "    print(str_policy)\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "#  Optional Helpers  #\n",
    "######################\n",
    "\n",
    "# Here we provide some helper functions simply for your convinience.\n",
    "# You DON'T necessarily need them, especially \"env_wrapper\" if\n",
    "# you want to deal with it in your different ways.\n",
    "\n",
    "# Feel FREE to change/delete these helper functions.\n",
    "\n",
    "def display_policy_letters(env, policy):\n",
    "    \"\"\"Displays a policy as letters, as required by problem 1.2 & 1.3\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "    policy: np.ndarray, with shape (env.nS)\n",
    "    \"\"\"\n",
    "    policy_letters = []\n",
    "    for l in policy:\n",
    "        policy_letters.append(lake_env.action_names[l][0])\n",
    "\n",
    "    policy_letters = np.array(policy_letters).reshape(env.nrow, env.ncol)\n",
    "\n",
    "\n",
    "    for row in range(env.nrow):\n",
    "        print(''.join(policy_letters[row, :]))\n",
    "    # Return an array \n",
    "    return policy_letters\n",
    "\n",
    "def env_wrapper(env_name):\n",
    "    \"\"\"Create a convinent wrapper for the loaded environment\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "\n",
    "    Usage e.g.:\n",
    "    ----------\n",
    "        envd4 = env_load('Deterministic-4x4-FrozenLake-v0')\n",
    "        envd8 = env_load('Deterministic-8x8-FrozenLake-v0')\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # T : the transition probability from s to sâ€™ via action a\n",
    "    # R : the reward you get when moving from s to s' via action a\n",
    "    env.T = np.zeros((env.nS, env.nA, env.nS))\n",
    "    env.R = np.zeros((env.nS, env.nA, env.nS))\n",
    "\n",
    "    for state in range(env.nS):\n",
    "        for action in range(env.nA):\n",
    "            for prob, nextstate, reward, is_terminal in env.P[state][action]:\n",
    "                env.T[state, action, nextstate] = prob\n",
    "                env.R[state, action, nextstate] = reward\n",
    "    return env\n",
    "\n",
    "\n",
    "def value_func_heatmap(env, value_func):\n",
    "    \"\"\"Visualize a policy as a heatmap, as required by problem 1.2 & 1.3\n",
    "\n",
    "    Note that you might need:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "    value_func: np.ndarray, with shape (env.nS)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    sns.heatmap(np.reshape(value_func, [env.nrow, env.ncol]),\n",
    "                annot=False, linewidths=.5, cmap=\"GnBu_r\", ax=ax,\n",
    "                yticklabels = np.arange(1, env.nrow+1)[::-1],\n",
    "                xticklabels = np.arange(1, env.nrow+1))\n",
    "    # Other choices of cmap: YlGnBu\n",
    "    # More: https://matplotlib.org/3.1.1/gallery/color/colormap_reference.html\n",
    "    return fig\n",
    "\n",
    "def write_answers(env, policy, env_name, policy_name, value_func, output_path=Path('./outputs')):\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    value_map = value_func_heatmap(env, value_func)\n",
    "    value_map.savefig(output_path/(env_name+'-'+policy_name+'.png'))\n",
    "    policy_actions_arr = display_policy_letters(env, policy)\n",
    "    np.savetxt(output_path/(env_name+'-'+policy_name+\"_policy_letters.txt\"), policy_actions_arr, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20b024-cf41-421b-9b43-42c25a21b679",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "acc11e0f-b922-4bb5-889d-e44c16896446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Vs(env, V, s, a, gamma=0.9):\n",
    "    ''' Cacluate V(s) given by the q(s,a) '''\n",
    "    r_tot = 0\n",
    "    for (p, s_next, r, _) in env.P[s][a]:\n",
    "        r_tot += p * (r + gamma * V[s_next])\n",
    "    return r_tot\n",
    "\n",
    "def value_function_to_policy(env, gamma, value_function):\n",
    "    \"\"\"Output action numbers for each state in value_function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to compute policy for. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    gamma: float\n",
    "      Discount factor. Number in range [0, 1)\n",
    "    value_function: np.ndarray\n",
    "      Value of each state.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "      An array of integers. Each integer is the optimal action to take\n",
    "      in that state according to the environment dynamics and the\n",
    "      given value function.\n",
    "    \"\"\"\n",
    "    # Hint: You might want to first calculate Q value,\n",
    "    #       and then take the argmax.\n",
    "    pi = np.zeros(env.nS, dtype=int)\n",
    "    for s in range(env.nS):\n",
    "        a_v = [compute_Vs(env, value_function, s, a, gamma=gamma) for a in range(env.nA)]\n",
    "        pi[s] = np.argmax(a_v)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac28937-6552-4912-9927-fa488dda4b87",
   "metadata": {},
   "source": [
    "Policy iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b208d65a-d81e-4683-88e6-c01d6671dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_sync(env, value_func, gamma, policy, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Performs policy evaluation.\n",
    "\n",
    "    Evaluates the value of a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    value_func: np.array\n",
    "      The current value functione estimate\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    policy: np.array\n",
    "      The policy to evaluate. Maps states to actions.\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, int\n",
    "      The value for the given policy and the number of iterations till\n",
    "      the value function converged.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_eval_iter = 0\n",
    "    print(\"Sync Policy evaluation\")\n",
    "    while True:\n",
    "        delta = 0\n",
    "        n_eval_iter+=1; print(\"Evluation iteration:\", n_eval_iter)\n",
    "        v_s_old = value_func.copy() # make a copy (synchronous require 2 distinct copy)\n",
    "        for s in range(env.nS):\n",
    "            value_func[s] = compute_Vs(env, V=v_s_old, s=s, a = policy[s], gamma=gamma)\n",
    "            delta = max(delta, np.abs(value_func[s] - v_s_old[s]))\n",
    "        if delta < tol or n_eval_iter==max_iterations: \n",
    "            return value_func, n_eval_iter\n",
    "\n",
    "def evaluate_policy_async_ordered(env, value_func, gamma, policy, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Performs policy evaluation.\n",
    "\n",
    "    Evaluates the value of a given policy by asynchronous DP.  Updates states in\n",
    "    their 1-N order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    value_func: np.array\n",
    "      The current value functione estimate\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    policy: np.array\n",
    "      The policy to evaluate. Maps states to actions.\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, int\n",
    "      The value for the given policy and the number of iterations till\n",
    "      the value function converged.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_eval_iter = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        n_eval_iter+=1; print(\"Evluation iteration:\", n_eval_iter)\n",
    "        for s in range(env.nS):\n",
    "            old_v = value_func[s]\n",
    "            value_func[s] = compute_Vs(env, V=value_func, s=s, a=policy[s], gamma=gamma)   # use old value function\n",
    "            delta = max(delta, np.abs(value_func[s] - old_v))\n",
    "        print(\"Iteration delta:\", delta)\n",
    "        if delta < tol: return value_func, n_eval_iter\n",
    "        if n_eval_iter >= max_iterations: return value_func, n_eval_iter\n",
    "\n",
    "def evaluate_policy_async_randperm(env, value_func, gamma, policy, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Performs policy evaluation.\n",
    "\n",
    "    Evaluates the value of a policy.  Updates states by randomly sampling index\n",
    "    order permutations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    value_func: np.array\n",
    "      The current value functione estimate\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    policy: np.array\n",
    "      The policy to evaluate. Maps states to actions.\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, int\n",
    "      The value for the given policy and the number of iterations till\n",
    "      the value function converged.\n",
    "    \"\"\"\n",
    "    n_eval_iter = 0\n",
    "    while True:\n",
    "        n_eval_iter+=1; print(\"Evluation iteration:\", n_eval_iter)\n",
    "        delta = 0\n",
    "        for s in random.sample(range(env.nS), env.nS):\n",
    "            v_old = value_func[s]\n",
    "            value_func[s] = compute_Vs(env, value_func, s, a=policy[s], gamma=gamma)\n",
    "            delta = max(delta, np.abs(value_func[s] - v_old))\n",
    "        if delta < tol: return value_func, n_eval_iter \n",
    "        if n_eval_iter >= max_iterations: return value_func, n_eval_iter\n",
    "\n",
    "def improve_policy(env, value_func, policy, gamma=0.9):\n",
    "    \"\"\"Performs policy improvement.\n",
    "\n",
    "    Given a policy and value function, improves the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    value_func: np.ndarray\n",
    "      Value function for the given policy.\n",
    "    policy: dict or np.array\n",
    "      The policy to improve. Maps states to actions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool, np.ndarray\n",
    "      Returns true if policy changed. Also returns the new policy.\n",
    "    \"\"\"\n",
    "    print(\"Improve Policy\")\n",
    "    policy_stable=False\n",
    "    old_policy = policy.copy()\n",
    "    for s in range(env.nS): # iterate for each state\n",
    "        a_r = [compute_Vs(env, value_func, s, a, gamma) for a in range(env.nA)] # action reward\n",
    "        policy[s] = np.argwhere(a_r==np.max(a_r)).flatten()[0]\n",
    "    if np.all(policy == old_policy): policy_stable = True\n",
    "    #print(policy, old_policy)\n",
    "    return policy_stable, policy\n",
    "\n",
    "\n",
    "def policy_iteration_sync(env, gamma=0.9, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    See page 85 of the Sutton & Barto Second Edition book.\n",
    "\n",
    "    You should use the improve_policy() and evaluate_policy_sync() methods to\n",
    "    implement this method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (np.ndarray, np.ndarray, int, int)\n",
    "       Returns optimal policy, value function, number of policy\n",
    "       improvement iterations, and number of value iterations.\n",
    "    \"\"\"\n",
    "    value_func = np.random.uniform(low=0, high=0, size=env.nS)\n",
    "    pi = np.zeros(env.nS, dtype=int)\n",
    "    stable=False\n",
    "    n_eval_iter = n_iter = 0\n",
    "    while not stable:\n",
    "        n_iter += 1; print(\"Iteration number:\", n_iter)\n",
    "        value_func, eval_iters = evaluate_policy_sync(env, value_func, gamma, pi,max_iterations, tol)\n",
    "        stable, pi = improve_policy(env, value_func, pi, gamma)\n",
    "        n_eval_iter += eval_iters\n",
    "    return pi, value_func, n_iter, n_eval_iter\n",
    "\n",
    "\n",
    "def policy_iteration_async_ordered(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    You should use the improve_policy and evaluate_policy_async_ordered methods\n",
    "    to implement this method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (np.ndarray, np.ndarray, int, int)\n",
    "       Returns optimal policy, value function, number of policy\n",
    "       improvement iterations, and number of value iterations.\n",
    "    \"\"\"\n",
    "    value_func = np.random.uniform(low=0, high=0, size=env.nS)\n",
    "    pi = np.zeros(env.nS, dtype=int)\n",
    "    stable=False\n",
    "    n_eval_iter = n_iter = 0\n",
    "    while not stable:\n",
    "        n_iter += 1; print(\"Iteration number:\", n_iter)\n",
    "        value_func, eval_iters = evaluate_policy_async_ordered(env, value_func, gamma, pi,max_iterations, tol)\n",
    "        stable, pi = improve_policy(env, value_func, pi, gamma)\n",
    "        n_eval_iter += eval_iters\n",
    "    return pi, value_func, n_iter, n_eval_iter\n",
    "\n",
    "\n",
    "def policy_iteration_async_randperm(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    You should use the improve_policy and evaluate_policy_async_randperm methods\n",
    "    to implement this method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (np.ndarray, np.ndarray, int, int)\n",
    "       Returns optimal policy, value function, number of policy\n",
    "       improvement iterations, and number of value iterations.\n",
    "    \"\"\"\n",
    "    value_func = np.random.uniform(low=0, high=0, size=env.nS)\n",
    "    pi = np.zeros(env.nS, dtype=int)\n",
    "    stable=False\n",
    "    n_val_iter = n_iter = 0\n",
    "    while not stable:\n",
    "        n_iter += 1; print(\"Iteration number:\", n_iter)\n",
    "        value_func, eval_iters = evaluate_policy_async_randperm(env, value_func, gamma, pi,max_iterations, tol)\n",
    "        stable, pi = improve_policy(env, value_func, pi, gamma)\n",
    "        n_val_iter += eval_iters\n",
    "    return pi, value_func, n_iter, n_val_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fb7d003e-fbe0-4836-9f27-8ef3ec2988a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Took avg total iterations:8.0\n",
      "Took avg evaluation iterations:16.5\n",
      "DLRD\n",
      "RLLD\n",
      "ULLD\n",
      "ULLL\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    envs = ['Deterministic-4x4-FrozenLake-v0', 'Deterministic-8x8-FrozenLake-v0']\n",
    "    policy_iteration_types = {'policy_iteration_sync': policy_iteration_sync, \n",
    "                              'policy_iteration_async_ordered': policy_iteration_async_ordered,\n",
    "                              'policy_iteration_async_randperm': policy_iteration_async_randperm}\n",
    "    # Define num_trials, gamma and whatever variables you need below.\n",
    "    env_num = 0\n",
    "    policy_iter_type = 'policy_iteration_async_randperm'\n",
    "    env = gym.make(envs[env_num])\n",
    " \n",
    "    # Define num_trials, gamma and whatever variables you need below.\n",
    "    max_trials = 1e3\n",
    "    tol=1e-3\n",
    "    gamma = .9\n",
    "    n_trials = 10\n",
    "    all_iters, all_eval_iters = [], []\n",
    "    for i in range(n_trials):\n",
    "        print(\"Policy iteration\")\n",
    "        policy, value_func, n_pi_iterations, n_eval_iteration = policy_iteration_types[policy_iter_type](env, gamma=gamma)\n",
    "        all_iters.append(n_pi_iterations)\n",
    "        all_eval_iters.append(n_eval_iteration)\n",
    "    print(f\"Took avg total iterations:{sum(all_iters)/n_trials}\")\n",
    "    print(f\"Took avg evaluation iterations:{sum(all_eval_iters)/n_trials}\")\n",
    "    write_answers(env, policy, envs[env_num], policy_iter_type, value_func)\n",
    "    #pi_ = value_function_to_policy(env, gamma, value_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab06ce-9cf4-4bfe-82f4-6c54ae1a32be",
   "metadata": {},
   "source": [
    "<b>Value iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "78c9fa64-e03b-4446-912a-fec440a00a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_sync(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs value iteration for a given gamma and environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, iteration\n",
    "      The value function and the number of iterations it took to converge.\n",
    "    \"\"\"\n",
    "    value_func = np.zeros(env.nS)  # initialize value function\n",
    "    #value_func = np.random.uniform(low=0, high=0, size=env.nS)\n",
    "    n_iter=0\n",
    "    while True:\n",
    "        n_iter+=1; print(\"Evluation iteration:\", n_iter)\n",
    "        delta = 0\n",
    "        value_func_old = value_func.copy()\n",
    "        for s in range(env.nS): # iterate for each state\n",
    "            action_vals = [compute_Vs(env, value_func_old, s, a, gamma=gamma) for a in range(env.nA)]\n",
    "            value_func[s] = np.max(action_vals)\n",
    "            delta = max(delta, np.abs(value_func[s] - value_func_old[s]))\n",
    "        if delta < tol: return value_func, n_iter\n",
    "        if n_iter > max_iterations: return value_func, n_iter\n",
    "\n",
    "\n",
    "\n",
    "def value_iteration_async_ordered(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs value iteration for a given gamma and environment.\n",
    "    Updates states in their 1-N order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, iteration\n",
    "      The value function and the number of iterations it took to converge.\n",
    "    \"\"\"\n",
    "    value_func = np.zeros(env.nS)  # initialize value function\n",
    "    n_iter=0\n",
    "    while True:\n",
    "        n_iter+=1; print(\"Evluation iteration:\", n_iter)\n",
    "        delta = 0\n",
    "        for s in range(env.nS): # iterate for each state\n",
    "            old_val = value_func[s]\n",
    "            value_func[s] = np.max([compute_Vs(env, value_func, s, a, gamma=gamma) for a in range(env.nA)])\n",
    "            delta = max(delta, np.abs(value_func[s] - old_val))\n",
    "        if delta < tol: return value_func, n_iter\n",
    "        if n_iter > max_iterations: return value_func, n_iter\n",
    "    return value_func, 0\n",
    "\n",
    "\n",
    "def value_iteration_async_randperm(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs value iteration for a given gamma and environment.\n",
    "    Updates states by randomly sampling index order permutations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, iteration\n",
    "      The value function and the number of iterations it took to converge.\n",
    "    \"\"\"    \n",
    "    value_func = np.zeros(env.nS)  # initialize value function\n",
    "    n_iter=0\n",
    "    while True:\n",
    "        n_iter+=1; print(\"Evluation iteration:\", n_iter)\n",
    "        delta = 0\n",
    "        for s in random.sample(range(env.nS), env.nS): # iterate for each state\n",
    "            old_val = value_func[s]\n",
    "            value_func[s] = np.max([compute_Vs(env, value_func, s, a, gamma=gamma) for a in range(env.nA)])\n",
    "            delta = max(delta, np.abs(value_func[s] - old_val))\n",
    "        if delta < tol: return value_func, n_iter\n",
    "        if n_iter > max_iterations: return value_func, n_iter\n",
    "    return value_func, 0\n",
    "\n",
    "\n",
    "def value_iteration_async_custom(env, gamma, goal_coord, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs value iteration for a given gamma and environment.\n",
    "    Updates states by student-defined heuristic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, iteration\n",
    "      The value function and the number of iterations it took to converge.\n",
    "    \"\"\"\n",
    "    def get_matrix_dist_from_pt(n, start_coord=(0,0)):\n",
    "        \"\"\"\n",
    "        get distance matrix from a starting point\n",
    "        \"\"\"\n",
    "        dist_mat = np.zeros((n,n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                dist_mat[i][j] = np.sqrt((i-start_coord[0])**2+(j-start_coord[1])**2)\n",
    "        return dist_mat\n",
    "\n",
    "    value_func = np.zeros(env.nS)  # initialize value function\n",
    "    dist_ordered_states = np.argsort(get_matrix_dist_from_pt(int(np.sqrt(env.nS)), start_coord=goal_coord).flatten())\n",
    "    n_iter=0\n",
    "    while True:\n",
    "        n_iter+=1; print(\"Evluation iteration:\", n_iter)\n",
    "        delta = 0\n",
    "        for s in dist_ordered_states: # iterate for each state\n",
    "            old_val = value_func[s]\n",
    "            value_func[s] = np.max([compute_Vs(env, value_func, s, a, gamma=gamma) for a in range(env.nA)])\n",
    "            delta = max(delta, np.abs(value_func[s] - old_val))\n",
    "        if delta < tol: return value_func, n_iter\n",
    "        if n_iter > max_iterations: return value_func, n_iter\n",
    "    return value_func, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d557f449-50f9-4ee8-83d9-6bcdbd80e937",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Evluation iteration: 6\n",
      "Evluation iteration: 7\n",
      "Value iteration\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Evluation iteration: 6\n",
      "Evluation iteration: 7\n",
      "Value iteration\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Evluation iteration: 6\n",
      "Evluation iteration: 7\n",
      "Value iteration\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Evluation iteration: 6\n",
      "Evluation iteration: 7\n",
      "Value iteration\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Evluation iteration: 6\n",
      "Evluation iteration: 7\n",
      "Value iteration\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Evluation iteration: 6\n",
      "Evluation iteration: 7\n",
      "Value iteration\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Evluation iteration: 6\n",
      "Evluation iteration: 7\n",
      "Value iteration\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Evluation iteration: 6\n",
      "Evluation iteration: 7\n",
      "Value iteration\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Evluation iteration: 6\n",
      "Evluation iteration: 7\n",
      "Value iteration\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Evluation iteration: 6\n",
      "Evluation iteration: 7\n",
      "Took avg total iterations:7.0\n",
      "DDDDDDDL\n",
      "RRRRRRDL\n",
      "LLLLLLDL\n",
      "DLDLLLLL\n",
      "DLDLLLLL\n",
      "DLDLLLLU\n",
      "DLDLLDLL\n",
      "RLLLLLLL\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    envs = ['Deterministic-4x4-FrozenLake-v0', 'Deterministic-8x8-FrozenLake-v0']\n",
    "    goal_coord = {\n",
    "        'Deterministic-4x4-FrozenLake-v0': (1, 1),\n",
    "        'Deterministic-8x8-FrozenLake-v0': (7, 1)\n",
    "    }\n",
    "    value_iteration_types = {\n",
    "        'value_iteration_sync': value_iteration_sync, \n",
    "        'value_iteration_async_ordered': value_iteration_async_ordered,\n",
    "        'value_iteration_async_randperm': value_iteration_async_randperm,\n",
    "        'value_iteration_async_custom': value_iteration_async_custom\n",
    "    }\n",
    "    # Define num_trials, gamma and whatever variables you need below.\n",
    "    env_num = 1\n",
    "    value_iter_type = 'value_iteration_async_custom'\n",
    "    env = gym.make(envs[env_num])\n",
    " \n",
    "    # Define num_trials, gamma and whatever variables you need below.\n",
    "    max_trials = 1e3\n",
    "    tol=1e-3\n",
    "    gamma = .9\n",
    "    n_trials = 10\n",
    "    all_iters = []\n",
    "    for i in range(n_trials):\n",
    "        print(\"Value iteration\")\n",
    "        if value_iter_type == 'value_iteration_async_custom':\n",
    "            value_func, n_val_iter = value_iteration_types['value_iteration_async_custom'](\n",
    "                env, gamma, goal_coord=goal_coord[envs[env_num]])\n",
    "        else:\n",
    "            value_func, n_val_iter = value_iteration_types[value_iter_type](env, gamma)\n",
    "        pi = value_function_to_policy(env, gamma, value_func)\n",
    "        all_iters.append(n_val_iter)\n",
    "    print(f\"Took avg total iterations:{sum(all_iters)/n_trials}\")    \n",
    "    \n",
    "    write_answers(env, pi, envs[env_num], value_iter_type, value_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "02e9e2a5-1363-4c2b-bb96-2b5900627512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.value_iteration_async_custom(env, gamma, max_iterations=1000, tol=0.001)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration_types['value_iteration_async_custom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78e161-389c-4fdf-a69f-871590cbd29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ptml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "751f8404fd9ba6fa422a13228a83fafc63c6a677f401437aedd740cd4a5430cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
