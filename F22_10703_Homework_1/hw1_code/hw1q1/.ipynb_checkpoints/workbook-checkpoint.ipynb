{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36290b38-4326-48b7-b152-d0c048938e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge black\n",
    "# !pip install nb_black\n",
    "# !conda install -c conda-forge jupyterlab_code_formatter\n",
    "# %load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e6ac205-7c9f-4fd4-a665-2ff2ef3eda66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from __future__ import division, absolute_import\n",
    "from __future__ import print_function, unicode_literals\n",
    "\n",
    "import numpy as np, seaborn as sns, matplotlib.pyplot as plt, random\n",
    "from pathlib import Path\n",
    "import gym, lake_envs as lake_env\n",
    "\n",
    "\n",
    "def print_policy(policy, action_names):\n",
    "    \"\"\"Print the policy in human-readable format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy: np.ndarray\n",
    "      Array of state to action number mappings\n",
    "    action_names: dict\n",
    "      Mapping of action numbers to characters representing the action.\n",
    "    \"\"\"\n",
    "    str_policy = policy.astype('str')\n",
    "    for action_num, action_name in action_names.items():\n",
    "        np.place(str_policy, policy == action_num, action_name)\n",
    "    print(str_policy)\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "#  Optional Helpers  #\n",
    "######################\n",
    "\n",
    "# Here we provide some helper functions simply for your convinience.\n",
    "# You DON'T necessarily need them, especially \"env_wrapper\" if\n",
    "# you want to deal with it in your different ways.\n",
    "\n",
    "# Feel FREE to change/delete these helper functions.\n",
    "\n",
    "def display_policy_letters(env, policy):\n",
    "    \"\"\"Displays a policy as letters, as required by problem 1.2 & 1.3\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "    policy: np.ndarray, with shape (env.nS)\n",
    "    \"\"\"\n",
    "    policy_letters = []\n",
    "    for l in policy:\n",
    "        policy_letters.append(lake_env.action_names[l][0])\n",
    "\n",
    "    policy_letters = np.array(policy_letters).reshape(env.nrow, env.ncol)\n",
    "\n",
    "\n",
    "    for row in range(env.nrow):\n",
    "        print(''.join(policy_letters[row, :]))\n",
    "    # Return an array \n",
    "    return policy_letters\n",
    "\n",
    "def env_wrapper(env_name):\n",
    "    \"\"\"Create a convinent wrapper for the loaded environment\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "\n",
    "    Usage e.g.:\n",
    "    ----------\n",
    "        envd4 = env_load('Deterministic-4x4-FrozenLake-v0')\n",
    "        envd8 = env_load('Deterministic-8x8-FrozenLake-v0')\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # T : the transition probability from s to sâ€™ via action a\n",
    "    # R : the reward you get when moving from s to s' via action a\n",
    "    env.T = np.zeros((env.nS, env.nA, env.nS))\n",
    "    env.R = np.zeros((env.nS, env.nA, env.nS))\n",
    "\n",
    "    for state in range(env.nS):\n",
    "        for action in range(env.nA):\n",
    "            for prob, nextstate, reward, is_terminal in env.P[state][action]:\n",
    "                env.T[state, action, nextstate] = prob\n",
    "                env.R[state, action, nextstate] = reward\n",
    "    return env\n",
    "\n",
    "\n",
    "def value_func_heatmap(env, value_func):\n",
    "    \"\"\"Visualize a policy as a heatmap, as required by problem 1.2 & 1.3\n",
    "\n",
    "    Note that you might need:\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "    value_func: np.ndarray, with shape (env.nS)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    sns.heatmap(np.reshape(value_func, [env.nrow, env.ncol]),\n",
    "                annot=False, linewidths=.5, cmap=\"GnBu_r\", ax=ax,\n",
    "                yticklabels = np.arange(1, env.nrow+1)[::-1],\n",
    "                xticklabels = np.arange(1, env.nrow+1))\n",
    "    # Other choices of cmap: YlGnBu\n",
    "    # More: https://matplotlib.org/3.1.1/gallery/color/colormap_reference.html\n",
    "    return fig\n",
    "\n",
    "def write_answers(env, policy, env_name, policy_name, value_func, output_path=Path('./outputs')):\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    value_map = value_func_heatmap(env, value_func)\n",
    "    value_map.savefig(output_path/(env_name+'-'+policy_name+'.png'))\n",
    "    policy_actions_arr = display_policy_letters(env, policy)\n",
    "    np.savetxt(output_path/(env_name+'-'+policy_name+\"_policy_letters.txt\"), policy_actions_arr, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd20b024-cf41-421b-9b43-42c25a21b679",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "acc11e0f-b922-4bb5-889d-e44c16896446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Vs(env, V, s, a, gamma=0.9):\n",
    "    ''' Cacluate V(s) given by the q(s,a) '''\n",
    "    r_tot = 0\n",
    "    for (p, s_next, r, _) in env.P[s][a]:\n",
    "        r_tot += p * (r + gamma * V[s_next])\n",
    "    return r_tot\n",
    "\n",
    "def value_function_to_policy(env, gamma, value_function):\n",
    "    \"\"\"Output action numbers for each state in value_function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to compute policy for. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    gamma: float\n",
    "      Discount factor. Number in range [0, 1)\n",
    "    value_function: np.ndarray\n",
    "      Value of each state.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "      An array of integers. Each integer is the optimal action to take\n",
    "      in that state according to the environment dynamics and the\n",
    "      given value function.\n",
    "    \"\"\"\n",
    "    # Hint: You might want to first calculate Q value,\n",
    "    #       and then take the argmax.\n",
    "    pi = np.zeros(env.nS, dtype=int)\n",
    "    for s in range(env.nS):\n",
    "        a_v = [compute_Vs(env, value_function, s, a, gamma=gamma) for a in range(env.nA)]\n",
    "        pi[s] = np.argmax(a_v)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac28937-6552-4912-9927-fa488dda4b87",
   "metadata": {},
   "source": [
    "Policy iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b208d65a-d81e-4683-88e6-c01d6671dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_sync(env, value_func, gamma, policy, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Performs policy evaluation.\n",
    "\n",
    "    Evaluates the value of a given policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    value_func: np.array\n",
    "      The current value functione estimate\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    policy: np.array\n",
    "      The policy to evaluate. Maps states to actions.\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, int\n",
    "      The value for the given policy and the number of iterations till\n",
    "      the value function converged.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_eval_iter = 0\n",
    "    print(\"Sync Policy evaluation\")\n",
    "    while True:\n",
    "        delta = 0\n",
    "        n_eval_iter+=1; print(\"Evluation iteration:\", n_eval_iter)\n",
    "        v_s_old = value_func.copy() # make a copy (synchronous require 2 distinct copy)\n",
    "        for s in range(env.nS):\n",
    "            value_func[s] = compute_Vs(env, V=v_s_old, s=s, a = policy[s], gamma=gamma)\n",
    "            delta = max(delta, np.abs(value_func[s] - v_s_old[s]))\n",
    "        if delta < tol or n_eval_iter==max_iterations: \n",
    "            return value_func, n_eval_iter\n",
    "\n",
    "def evaluate_policy_async_ordered(env, value_func, gamma, policy, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Performs policy evaluation.\n",
    "\n",
    "    Evaluates the value of a given policy by asynchronous DP.  Updates states in\n",
    "    their 1-N order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    value_func: np.array\n",
    "      The current value functione estimate\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    policy: np.array\n",
    "      The policy to evaluate. Maps states to actions.\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, int\n",
    "      The value for the given policy and the number of iterations till\n",
    "      the value function converged.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_eval_iter = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        n_eval_iter+=1; print(\"Evluation iteration:\", n_eval_iter)\n",
    "        for s in range(env.nS):\n",
    "            old_v = value_func[s]\n",
    "            value_func[s] = compute_Vs(env, V=value_func, s=s, a=policy[s], gamma=gamma)   # use old value function\n",
    "            delta = max(delta, np.abs(value_func[s] - old_v))\n",
    "        print(\"Iteration delta:\", delta)\n",
    "        if delta < tol: return value_func, n_eval_iter\n",
    "        if n_eval_iter >= max_iterations: return value_func, n_eval_iter\n",
    "\n",
    "def evaluate_policy_async_randperm(env, value_func, gamma, policy, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Performs policy evaluation.\n",
    "\n",
    "    Evaluates the value of a policy.  Updates states by randomly sampling index\n",
    "    order permutations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    value_func: np.array\n",
    "      The current value functione estimate\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    policy: np.array\n",
    "      The policy to evaluate. Maps states to actions.\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, int\n",
    "      The value for the given policy and the number of iterations till\n",
    "      the value function converged.\n",
    "    \"\"\"\n",
    "    n_eval_iter = 0\n",
    "    while True:\n",
    "        n_eval_iter+=1; print(\"Evluation iteration:\", n_eval_iter)\n",
    "        delta = 0\n",
    "        for s in random.sample(range(env.nS), env.nS):\n",
    "            v_old = value_func[s]\n",
    "            value_func[s] = compute_Vs(env, value_func, s, a=policy[s], gamma=gamma)\n",
    "            delta = max(delta, np.abs(value_func[s] - v_old))\n",
    "        if delta < tol: return value_func, n_eval_iter \n",
    "        if n_eval_iter >= max_iterations: return value_func, n_eval_iter\n",
    "\n",
    "def improve_policy(env, value_func, policy, gamma=0.9):\n",
    "    \"\"\"Performs policy improvement.\n",
    "\n",
    "    Given a policy and value function, improves the policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    value_func: np.ndarray\n",
    "      Value function for the given policy.\n",
    "    policy: dict or np.array\n",
    "      The policy to improve. Maps states to actions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool, np.ndarray\n",
    "      Returns true if policy changed. Also returns the new policy.\n",
    "    \"\"\"\n",
    "    print(\"Improve Policy\")\n",
    "    policy_stable=False\n",
    "    old_policy = policy.copy()\n",
    "    for s in range(env.nS): # iterate for each state\n",
    "        a_r = [compute_Vs(env, value_func, s, a, gamma) for a in range(env.nA)] # action reward\n",
    "        policy[s] = np.argwhere(a_r==np.max(a_r)).flatten()[0]\n",
    "    if np.all(policy == old_policy): policy_stable = True\n",
    "    #print(policy, old_policy)\n",
    "    return policy_stable, policy\n",
    "\n",
    "\n",
    "def policy_iteration_sync(env, gamma=0.9, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    See page 85 of the Sutton & Barto Second Edition book.\n",
    "\n",
    "    You should use the improve_policy() and evaluate_policy_sync() methods to\n",
    "    implement this method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (np.ndarray, np.ndarray, int, int)\n",
    "       Returns optimal policy, value function, number of policy\n",
    "       improvement iterations, and number of value iterations.\n",
    "    \"\"\"\n",
    "    value_func = np.random.uniform(low=0, high=0, size=env.nS)\n",
    "    pi = np.zeros(env.nS, dtype=int)\n",
    "    stable=False\n",
    "    n_eval_iter = n_iter = 0\n",
    "    while not stable:\n",
    "        n_iter += 1; print(\"Iteration number:\", n_iter)\n",
    "        value_func, eval_iters = evaluate_policy_sync(env, value_func, gamma, pi,max_iterations, tol)\n",
    "        stable, pi = improve_policy(env, value_func, pi, gamma)\n",
    "        n_eval_iter += eval_iters\n",
    "    return pi, value_func, n_iter, n_eval_iter\n",
    "\n",
    "\n",
    "def policy_iteration_async_ordered(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    You should use the improve_policy and evaluate_policy_async_ordered methods\n",
    "    to implement this method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (np.ndarray, np.ndarray, int, int)\n",
    "       Returns optimal policy, value function, number of policy\n",
    "       improvement iterations, and number of value iterations.\n",
    "    \"\"\"\n",
    "    value_func = np.random.uniform(low=0, high=0, size=env.nS)\n",
    "    pi = np.zeros(env.nS, dtype=int)\n",
    "    stable=False\n",
    "    n_eval_iter = n_iter = 0\n",
    "    while not stable:\n",
    "        n_iter += 1; print(\"Iteration number:\", n_iter)\n",
    "        value_func, eval_iters = evaluate_policy_async_ordered(env, value_func, gamma, pi,max_iterations, tol)\n",
    "        stable, pi = improve_policy(env, value_func, pi, gamma)\n",
    "        n_eval_iter += eval_iters\n",
    "    return pi, value_func, n_iter, n_eval_iter\n",
    "\n",
    "\n",
    "def policy_iteration_async_randperm(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    You should use the improve_policy and evaluate_policy_async_randperm methods\n",
    "    to implement this method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (np.ndarray, np.ndarray, int, int)\n",
    "       Returns optimal policy, value function, number of policy\n",
    "       improvement iterations, and number of value iterations.\n",
    "    \"\"\"\n",
    "    value_func = np.random.uniform(low=0, high=0, size=env.nS)\n",
    "    pi = np.zeros(env.nS, dtype=int)\n",
    "    stable=False\n",
    "    n_val_iter = n_iter = 0\n",
    "    while not stable:\n",
    "        n_iter += 1; print(\"Iteration number:\", n_iter)\n",
    "        value_func, eval_iters = evaluate_policy_async_randperm(env, value_func, gamma, pi,max_iterations, tol)\n",
    "        stable, pi = improve_policy(env, value_func, pi, gamma)\n",
    "        n_val_iter += eval_iters\n",
    "    return pi, value_func, n_iter, n_val_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fb7d003e-fbe0-4836-9f27-8ef3ec2988a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Evluation iteration: 5\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Policy iteration\n",
      "Iteration number: 1\n",
      "Evluation iteration: 1\n",
      "Improve Policy\n",
      "Iteration number: 2\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 3\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 4\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Evluation iteration: 3\n",
      "Evluation iteration: 4\n",
      "Improve Policy\n",
      "Iteration number: 5\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 6\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 7\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Iteration number: 8\n",
      "Evluation iteration: 1\n",
      "Evluation iteration: 2\n",
      "Improve Policy\n",
      "Took avg total iterations:8.0\n",
      "Took avg evaluation iterations:16.5\n",
      "DLRD\n",
      "RLLD\n",
      "ULLD\n",
      "ULLL\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    envs = ['Deterministic-4x4-FrozenLake-v0', 'Deterministic-8x8-FrozenLake-v0']\n",
    "    policy_iteration_types = {'policy_iteration_sync': policy_iteration_sync, \n",
    "                              'policy_iteration_async_ordered': policy_iteration_async_ordered,\n",
    "                              'policy_iteration_async_randperm': policy_iteration_async_randperm}\n",
    "    # Define num_trials, gamma and whatever variables you need below.\n",
    "    env_num = 0\n",
    "    policy_iter_type = 'policy_iteration_async_randperm'\n",
    "    env = gym.make(envs[env_num])\n",
    " \n",
    "    # Define num_trials, gamma and whatever variables you need below.\n",
    "    max_trials = 1e3\n",
    "    tol=1e-3\n",
    "    gamma = .9\n",
    "    n_trials = 10\n",
    "    all_iters, all_eval_iters = [], []\n",
    "    for i in range(n_trials):\n",
    "        print(\"Policy iteration\")\n",
    "        policy, value_func, n_pi_iterations, n_eval_iteration = policy_iteration_types[policy_iter_type](env, gamma=gamma)\n",
    "        all_iters.append(n_pi_iterations)\n",
    "        all_eval_iters.append(n_eval_iteration)\n",
    "    print(f\"Took avg total iterations:{sum(all_iters)/n_trials}\")\n",
    "    print(f\"Took avg evaluation iterations:{sum(all_eval_iters)/n_trials}\")\n",
    "    write_answers(env, policy, envs[env_num], policy_iter_type, value_func)\n",
    "    #pi_ = value_function_to_policy(env, gamma, value_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab06ce-9cf4-4bfe-82f4-6c54ae1a32be",
   "metadata": {},
   "source": [
    "<b>Value iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "78c9fa64-e03b-4446-912a-fec440a00a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_sync(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs value iteration for a given gamma and environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, iteration\n",
    "      The value function and the number of iterations it took to converge.\n",
    "    \"\"\"\n",
    "    value_func = np.zeros(env.nS)  # initialize value function\n",
    "    #value_func = np.random.uniform(low=0, high=0, size=env.nS)\n",
    "    n_iter=0\n",
    "    while True:\n",
    "        n_iter+=1; print(\"Evluation iteration:\", n_iter)\n",
    "        delta = 0\n",
    "        value_func_old = value_func.copy()\n",
    "        for s in range(env.nS): # iterate for each state\n",
    "            action_vals = [compute_Vs(env, value_func_old, s, a, gamma=gamma) for a in range(env.nA)]\n",
    "            value_func[s] = np.max(action_vals)\n",
    "            delta = max(delta, np.abs(value_func[s] - value_func_old[s]))\n",
    "        if delta < tol: return value_func, n_iter\n",
    "        if n_iter > max_iterations: return value_func, n_iter\n",
    "\n",
    "\n",
    "\n",
    "def value_iteration_async_ordered(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs value iteration for a given gamma and environment.\n",
    "    Updates states in their 1-N order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, iteration\n",
    "      The value function and the number of iterations it took to converge.\n",
    "    \"\"\"\n",
    "    value_func = np.zeros(env.nS)  # initialize value function\n",
    "    n_iter=0\n",
    "    while True:\n",
    "        n_iter+=1; print(\"Evluation iteration:\", n_iter)\n",
    "        delta = 0\n",
    "        for s in range(env.nS): # iterate for each state\n",
    "            old_val = value_func[s]\n",
    "            value_func[s] = np.max([compute_Vs(env, value_func, s, a, gamma=gamma) for a in range(env.nA)])\n",
    "            delta = max(delta, np.abs(value_func[s] - old_val))\n",
    "        if delta < tol: return value_func, n_iter\n",
    "        if n_iter > max_iterations: return value_func, n_iter\n",
    "    return value_func, 0\n",
    "\n",
    "\n",
    "def value_iteration_async_randperm(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs value iteration for a given gamma and environment.\n",
    "    Updates states by randomly sampling index order permutations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, iteration\n",
    "      The value function and the number of iterations it took to converge.\n",
    "    \"\"\"    \n",
    "    value_func = np.zeros(env.nS)  # initialize value function\n",
    "    n_iter=0\n",
    "    while True:\n",
    "        n_iter+=1; print(\"Evluation iteration:\", n_iter)\n",
    "        delta = 0\n",
    "        for s in random.sample(range(env.nS), env.nS): # iterate for each state\n",
    "            old_val = value_func[s]\n",
    "            value_func[s] = np.max([compute_Vs(env, value_func, s, a, gamma=gamma) for a in range(env.nA)])\n",
    "            delta = max(delta, np.abs(value_func[s] - old_val))\n",
    "        if delta < tol: return value_func, n_iter\n",
    "        if n_iter > max_iterations: return value_func, n_iter\n",
    "    return value_func, 0\n",
    "\n",
    "\n",
    "def value_iteration_async_custom(env, gamma, goal_coord, max_iterations=int(1e3), tol=1e-3):\n",
    "    \"\"\"Runs value iteration for a given gamma and environment.\n",
    "    Updates states by student-defined heuristic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      The environment to compute value iteration for. Must have nS,\n",
    "      nA, and P as attributes.\n",
    "    gamma: float\n",
    "      Discount factor, must be in range [0, 1)\n",
    "    max_iterations: int\n",
    "      The maximum number of iterations to run before stopping.\n",
    "    tol: float\n",
    "      Determines when value function has converged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray, iteration\n",
    "      The value function and the number of iterations it took to converge.\n",
    "    \"\"\"\n",
    "    def get_matrix_dist_from_pt(n, start_coord=(0,0)):\n",
    "        \"\"\"\n",
    "        get distance matrix from a starting point\n",
    "        \"\"\"\n",
    "        dist_mat = np.zeros((n,n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                dist_mat[i][j] = np.sqrt((i-start_coord[0])**2+(j-start_coord[1])**2)\n",
    "        return dist_mat\n",
    "\n",
    "    value_func = np.zeros(env.nS)  # initialize value function\n",
    "    dist_ordered_states = np.argsort(get_matrix_dist(int(np.sqrt(env.nS)), start_coord=goal_coord).flatten())\n",
    "    n_iter=0\n",
    "    while True:\n",
    "        n_iter+=1; print(\"Evluation iteration:\", n_iter)\n",
    "        delta = 0\n",
    "        for s in dist_ordered_states: # iterate for each state\n",
    "            old_val = value_func[s]\n",
    "            value_func[s] = np.max([compute_Vs(env, value_func, s, a, gamma=gamma) for a in range(env.nA)])\n",
    "            delta = max(delta, np.abs(value_func[s] - old_val))\n",
    "        if delta < tol: return value_func, n_iter\n",
    "        if n_iter > max_iterations: return value_func, n_iter\n",
    "    return value_func, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d557f449-50f9-4ee8-83d9-6bcdbd80e937",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "value_iteration_async_custom() got an unexpected keyword argument 'goal_coord'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19892/663620638.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Value iteration\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue_iter_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'value_iteration_async_custom'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             value_func, n_val_iter = value_iteration_types['value_iteration_async_custom'](\n\u001b[0m\u001b[0;32m     28\u001b[0m                 env, gamma, goal_coord=goal_coord[envs[env_num]])\n\u001b[0;32m     29\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: value_iteration_async_custom() got an unexpected keyword argument 'goal_coord'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    envs = ['Deterministic-4x4-FrozenLake-v0', 'Deterministic-8x8-FrozenLake-v0']\n",
    "    goal_coord = {\n",
    "        'Deterministic-4x4-FrozenLake-v0': (0, 2),\n",
    "        'Deterministic-8x8-FrozenLake-v0': (0, 5)\n",
    "    }\n",
    "    value_iteration_types = {\n",
    "        'value_iteration_sync': value_iteration_sync, \n",
    "        'value_iteration_async_ordered': value_iteration_async_ordered,\n",
    "        'value_iteration_async_randperm': value_iteration_async_randperm,\n",
    "        'value_iteration_async_custom': value_iteration_async_custom\n",
    "    }\n",
    "    # Define num_trials, gamma and whatever variables you need below.\n",
    "    env_num = 1\n",
    "    value_iter_type = 'value_iteration_async_custom'\n",
    "    env = gym.make(envs[env_num])\n",
    " \n",
    "    # Define num_trials, gamma and whatever variables you need below.\n",
    "    max_trials = 1e3\n",
    "    tol=1e-3\n",
    "    gamma = .9\n",
    "    n_trials = 10\n",
    "    all_iters = []\n",
    "    for i in range(n_trials):\n",
    "        print(\"Value iteration\")\n",
    "        if value_iter_type == 'value_iteration_async_custom':\n",
    "            value_func, n_val_iter = value_iteration_types['value_iteration_async_custom'](\n",
    "                env, gamma, goal_coord=goal_coord[envs[env_num]])\n",
    "        else:\n",
    "            value_func, n_val_iter = value_iteration_types[value_iter_type](env, gamma)\n",
    "        pi = value_function_to_policy(env, gamma, value_func)\n",
    "        all_iters.append(n_val_iter)\n",
    "    print(f\"Took avg total iterations:{sum(all_iters)/n_trials}\")    \n",
    "    \n",
    "    write_answers(env, pi, envs[env_num], value_iter_type, value_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "02e9e2a5-1363-4c2b-bb96-2b5900627512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.value_iteration_async_custom(env, gamma, max_iterations=1000, tol=0.001)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration_types['value_iteration_async_custom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78e161-389c-4fdf-a69f-871590cbd29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptml]",
   "language": "python",
   "name": "conda-env-ptml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
